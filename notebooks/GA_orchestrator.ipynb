{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# GSM8K GA Orchestrator\n", "End-to-end experiment runner and live monitor (population=50, 30 generations)."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "# Bootstrap config and environment\n",
    "from src.utils.config import load_config\n",
    "cfg = load_config()\n",
    "print(f'Using provider={cfg.model_provider}, model={cfg.model_name}, temp={cfg.temperature}, max_tokens={cfg.max_tokens}')\n",
    "print('Paths:', cfg.paths)\n",
    "print('Population size (config):', cfg.raw['population']['population_size'], 'Max generations:', cfg.raw['population']['max_generations'])\n",
    "print('Concurrency limit:', cfg.raw['evaluation']['concurrency_limit'])"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Data: Ensure GSM8K subsets exist"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "import os\n",
    "required = ['data/gsm8k_primary_eval.jsonl','data/gsm8k_validation.jsonl','data/gsm8k_final_test.jsonl']\n",
    "for p in required:\n",
    "    print(f'Found {p}:', os.path.exists(p))"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Seeds: curated 50 prompts preview"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "from src.genetics.seeds import SEED_PROMPTS\n",
    "print('Curated seeds:', len(SEED_PROMPTS))\n",
    "for i, s in enumerate(SEED_PROMPTS[:5]):\n",
    "    print(f'{i+1}.', s[:120])"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Run full 30-generation experiment (population=50) with live progress"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "import threading, time, json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from src.genetics.controller import evolve\n",
    "metrics_path = cfg.paths.get('logs','data/results/logs') + '/metrics.jsonl'\n",
    "# Start evolution in background thread\n",
    "def _run():\n",
    "    evolve()\n",
    "t = threading.Thread(target=_run)\n",
    "t.start()\n",
    "# Live monitor loop\n",
    "def read_metrics(path):\n",
    "    gens, bests, avgs, divs, hits, calls = [], [], [], [], [], []\n",
    "    best_text = None\n",
    "    if not os.path.exists(path):\n",
    "        return gens, bests, avgs, divs, hits, calls, best_text\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            gens.append(r.get('generation'))\n",
    "            bests.append(r.get('best_fitness'))\n",
    "            avgs.append(r.get('avg_fitness'))\n",
    "            divs.append(r.get('diversity'))\n",
    "            hits.append(r.get('cache_hit_rate', 0.0))\n",
    "            calls.append(r.get('api_calls', 0))\n",
    "            bt = r.get('best_text')\n",
    "            if bt: best_text = bt\n",
    "    return gens, bests, avgs, divs, hits, calls, best_text\n",
    "while t.is_alive():\n",
    "    clear_output(wait=True)\n",
    "    print('Running evolution...')\n",
    "    gens, bests, avgs, divs, hits, calls, best_text = read_metrics(metrics_path)\n",
    "    if gens:\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(14,8))\n",
    "        axs[0,0].plot(gens, bests); axs[0,0].set_title('Best Fitness');\n",
    "        axs[0,1].plot(gens, avgs); axs[0,1].set_title('Average Fitness');\n",
    "        axs[0,2].plot(gens, divs); axs[0,2].set_title('Diversity');\n",
    "        axs[1,0].plot(gens, hits); axs[1,0].set_title('Cache Hit Rate');\n",
    "        axs[1,1].plot(gens, calls); axs[1,1].set_title('API Calls');\n",
    "        axs[1,2].axis('off');\n",
    "        plt.tight_layout(); display(fig); plt.close(fig)\n",
    "        if best_text:\n",
    "            print('\nCurrent best prompt preview:\n', best_text[:400])\n",
    "    else:\n",
    "        print('Waiting for metrics...')\n",
    "    time.sleep(5)\n",
    "# Final draw\n",
    "clear_output(wait=True)\n",
    "gens, bests, avgs, divs, hits, calls, best_text = read_metrics(metrics_path)\n",
    "print('Evolution finished. Generations logged:', len(gens))\n",
    "if gens:\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(14,8))\n",
    "    axs[0,0].plot(gens, bests); axs[0,0].set_title('Best Fitness');\n",
    "    axs[0,1].plot(gens, avgs); axs[0,1].set_title('Average Fitness');\n",
    "    axs[0,2].plot(gens, divs); axs[0,2].set_title('Diversity');\n",
    "    axs[1,0].plot(gens, hits); axs[1,0].set_title('Cache Hit Rate');\n",
    "    axs[1,1].plot(gens, calls); axs[1,1].set_title('API Calls');\n",
    "    axs[1,2].axis('off'); plt.tight_layout(); display(fig); plt.close(fig)\n",
    "    if best_text:\n",
    "        print('\nFinal best prompt (truncated):\n', best_text[:800])\n"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Save final results"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "# Copy best prompt text to results directory if present in metrics\n",
    "import os, json, shutil\n",
    "metrics_path = cfg.paths.get('logs','data/results/logs') + '/metrics.jsonl'\n",
    "best_text = None\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line); bt = r.get('best_text');\n",
    "            if bt: best_text = bt\n",
    "if best_text:\n",
    "    outp = os.path.join(cfg.paths.get('results','data/results'), 'best_prompt_final.txt')\n",
    "    with open(outp, 'w') as f: f.write(best_text)\n",
    "    print('Saved final best prompt to', outp)\n",
    "else:\n",
    "    print('No best_text found in metrics.')\n"
  ]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.13"}},
 "nbformat": 4,
 "nbformat_minor": 5
}

