{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K Genetic Algorithm for Prompt Evolution\n",
    "\n",
    "## Complete Tutorial: Evolving Mathematical Reasoning Prompts\n",
    "\n",
    "This notebook provides a tutorial for using genetic algorithms to evolve prompts for mathematical reasoning on the GSM8K dataset. You'll learn how to:\n",
    "\n",
    "- Set up the system and configure experiments\n",
    "- Run evolution experiments with real-time monitoring\n",
    "- Analyze results and interpret evolved prompts\n",
    "- Customize parameters for different research goals\n",
    "\n",
    "**Prerequisites:**\n",
    "- OpenAI API key (for GPT models)\n",
    "- Anthropic API key (for Claude models) - optional\n",
    "- Python environment with required dependencies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Setup and Dependencies\n",
    "\n",
    "First, let's set up the environment and import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment and run if packages are not installed\n",
    "# install_package(\"openai>=1.0.0\")\n",
    "# install_package(\"anthropic\")\n",
    "# install_package(\"matplotlib\")\n",
    "# install_package(\"numpy\")\n",
    "# install_package(\"psutil\")\n",
    "\n",
    "print(\"âœ… Dependencies ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"ðŸ“ Project root: {project_root}\")\n",
    "print(\"âœ… System imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration\n",
    "\n",
    "Configure your API keys for accessing language models. The system supports both OpenAI and Anthropic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys\n",
    "# Option 1: Set environment variables (recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    with open(env_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "    print(\"âœ… Environment variables loaded from .env file\")\n",
    "else:\n",
    "    print(\"âš ï¸  No .env file found. Please set API keys manually.\")\n",
    "\n",
    "# Verify API keys are set\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(f\"ðŸ”‘ OpenAI API Key: {'âœ… Set' if openai_key else 'âŒ Not set'}\")\n",
    "print(f\"ðŸ”‘ Anthropic API Key: {'âœ… Set' if anthropic_key else 'âŒ Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load System Components\n",
    "\n",
    "Now let's load all the genetic algorithm components we'll need for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genetic algorithm components\n",
    "from src.utils.config import config\n",
    "from src.embeddings.vocabulary import vocabulary\n",
    "from src.seeds.seed_manager import SeedManager\n",
    "from src.config.experiment_configs import ConfigurationManager\n",
    "\n",
    "print(\"âœ… Core components imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vocabulary\n",
    "vocab_file = config.get_data_dir() / \"embeddings\" / \"vocabulary.pkl\"\n",
    "\n",
    "if vocab_file.exists():\n",
    "    vocabulary.load_vocabulary(vocab_file)\n",
    "    print(f\"âœ… Vocabulary loaded: {len(vocabulary.token_to_id)} tokens\")\n",
    "else:\n",
    "    print(\"ðŸ“š Creating vocabulary from scratch...\")\n",
    "    vocabulary._create_basic_vocabulary()\n",
    "    print(f\"âœ… Basic vocabulary created: {len(vocabulary.token_to_id)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seed manager and configuration manager\n",
    "seed_manager = SeedManager()\n",
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Load base seed collection\n",
    "base_seeds = seed_manager.get_base_seeds()\n",
    "print(f\"ðŸŒ± Seed collection loaded: {len(base_seeds)} high-quality prompts\")\n",
    "\n",
    "# Show available experiment presets\n",
    "presets = config_manager.list_presets()\n",
    "print(f\"âš™ï¸  Available presets: {', '.join(presets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Seed Prompts\n",
    "\n",
    "Let's examine the high-quality seed prompts that will initialize our genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show seed prompt categories and examples\n",
    "from src.seeds.prompt_categories import PromptCategory\n",
    "\n",
    "print(\"ðŸ“‚ Seed Prompt Categories:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for category in PromptCategory:\n",
    "    category_seeds = seed_manager.get_seeds_by_category(category)\n",
    "    print(f\"\\nðŸ”¹ {category.value.replace('_', ' ').title()}: {len(category_seeds)} prompts\")\n",
    "    \n",
    "    # Show first example\n",
    "    if category_seeds:\n",
    "        example = category_seeds[0]\n",
    "        print(f\"   Example: \\\"{example.text}\\\"\")\n",
    "        print(f\"   Strength: {example.expected_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate seed collection quality\n",
    "from src.seeds.seed_validation import SeedValidator\n",
    "\n",
    "validator = SeedValidator()\n",
    "validation_metrics = validator.validate_collection(base_seeds)\n",
    "\n",
    "print(\"ðŸ” Seed Collection Quality Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Overall Score: {validation_metrics.overall_score:.3f}\")\n",
    "print(f\"Diversity Score: {validation_metrics.diversity_score:.3f}\")\n",
    "print(f\"Category Balance: {validation_metrics.category_balance:.3f}\")\n",
    "print(f\"Uniqueness Score: {validation_metrics.uniqueness_score:.3f}\")\n",
    "\n",
    "quality_status = \"ðŸŸ¢ EXCELLENT\" if validation_metrics.overall_score >= 0.8 else \"ðŸŸ¡ GOOD\" if validation_metrics.overall_score >= 0.6 else \"ðŸ”´ NEEDS IMPROVEMENT\"\n",
    "print(f\"\\nQuality Status: {quality_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Your Experiment\n",
    "\n",
    "Now let's set up an experiment configuration. You can choose from predefined presets or customize parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available experiment presets\n",
    "preset_info = config_manager.get_preset_info()\n",
    "\n",
    "print(\"âš™ï¸  Available Experiment Presets:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, info in preset_info.items():\n",
    "    print(f\"\\nðŸ”¹ {name}\")\n",
    "    print(f\"   Name: {info['name']}\")\n",
    "    print(f\"   Description: {info['description']}\")\n",
    "    print(f\"   Population: {info['population_size']}, Generations: {info['max_generations']}\")\n",
    "    print(f\"   Problems: {info['max_problems']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and customize your experiment configuration\n",
    "# Options: 'quick_test', 'standard', 'thorough', 'high_mutation', 'large_population', etc.\n",
    "\n",
    "BASE_PRESET = \"quick_test\"  # Change this to your preferred preset\n",
    "\n",
    "# Custom modifications (optional)\n",
    "custom_modifications = {\n",
    "    'name': 'My GSM8K Evolution Experiment',\n",
    "    'description': 'Custom experiment for prompt evolution',\n",
    "    'population_size': 15,  # Adjust as needed\n",
    "    'max_generations': 20,  # Adjust as needed\n",
    "    'max_problems': 30,     # Adjust as needed (more problems = more accurate but slower)\n",
    "    'model_name': 'gpt-3.5-turbo',  # or 'gpt-4', 'claude-3-sonnet-20240229'\n",
    "    'temperature': 0.0,     # 0.0 for deterministic, higher for more creative\n",
    "    'target_fitness': 0.75, # Stop early if this fitness is reached\n",
    "}\n",
    "\n",
    "# Create the configuration\n",
    "experiment_config = config_manager.create_custom_config(BASE_PRESET, custom_modifications)\n",
    "\n",
    "# Show the final configuration\n",
    "print(\"ðŸ”§ Experiment Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(config_manager.get_config_summary(experiment_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the configuration\n",
    "validation_errors = config_manager.validate_config(experiment_config)\n",
    "\n",
    "if validation_errors:\n",
    "    print(\"âŒ Configuration validation failed:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"   - {error}\")\n",
    "else:\n",
    "    print(\"âœ… Configuration is valid and ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Monitoring and Visualization\n",
    "\n",
    "Before running the experiment, let's set up real-time monitoring and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import monitoring components\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "from src.utils.evolution_logging import EvolutionLogger\n",
    "from src.utils.visualization import EvolutionVisualizer\n",
    "from src.utils.performance_monitor import PerformanceMonitor\n",
    "\n",
    "# Initialize experiment manager\n",
    "experiment_manager = ExperimentManager()\n",
    "\n",
    "print(\"ðŸ“Š Monitoring components initialized\")\n",
    "print(\"âœ… Ready for experiment execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Evolution Experiment\n",
    "\n",
    "Now we'll run the complete genetic algorithm experiment with real-time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main experiment runner\n",
    "from src.main_runner import GSM8KExperimentRunner\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Convert configuration to dictionary format\n",
    "config_dict = asdict(experiment_config)\n",
    "\n",
    "# Convert enums to strings for JSON compatibility\n",
    "for key, value in config_dict.items():\n",
    "    if hasattr(value, 'value'):\n",
    "        config_dict[key] = value.value\n",
    "\n",
    "print(\"ðŸš€ Initializing experiment runner...\")\n",
    "runner = GSM8KExperimentRunner(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the experiment\n",
    "print(\"ðŸ”§ Setting up experiment components...\")\n",
    "setup_success = runner.setup_experiment()\n",
    "\n",
    "if setup_success:\n",
    "    print(\"âœ… Experiment setup completed successfully!\")\n",
    "    print(f\"ðŸ“‹ Experiment ID: {runner.experiment_id}\")\n",
    "else:\n",
    "    print(\"âŒ Experiment setup failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evolution experiment\n",
    "if setup_success:\n",
    "    print(\"ðŸ§¬ Starting genetic algorithm evolution...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Population Size: {experiment_config.population_size}\")\n",
    "    print(f\"Max Generations: {experiment_config.max_generations}\")\n",
    "    print(f\"Evaluation Problems: {experiment_config.max_problems}\")\n",
    "    print(f\"Model: {experiment_config.model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # This will run the complete evolution process\n",
    "    start_time = time.time()\n",
    "    experiment_success = runner.run_experiment()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Total experiment time: {total_time:.1f} seconds\")\n",
    "    \n",
    "    if experiment_success:\n",
    "        print(\"ðŸŽ‰ Experiment completed successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Experiment failed. Check the logs for details.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Skipping experiment run due to setup failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results\n",
    "\n",
    "Let's examine the results of our evolution experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment summary\n",
    "if 'experiment_success' in locals() and experiment_success:\n",
    "    summary = runner.get_experiment_summary()\n",
    "    \n",
    "    print(\"ðŸ“Š Experiment Results Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Status: {summary['status']}\")\n",
    "    print(f\"Experiment ID: {summary['experiment_id']}\")\n",
    "    \n",
    "    if 'results' in summary:\n",
    "        results = summary['results']\n",
    "        print(f\"\\nðŸ† Evolution Results:\")\n",
    "        print(f\"   Best Fitness: {results.get('best_fitness', 0):.3f}\")\n",
    "        print(f\"   Total Generations: {results.get('total_generations', 0)}\")\n",
    "        print(f\"   Convergence Reason: {results.get('convergence_reason', 'unknown')}\")\n",
    "        print(f\"   Total Evaluations: {results.get('total_evaluations', 0)}\")\n",
    "        \n",
    "        if summary.get('best_prompt'):\n",
    "            print(f\"\\nðŸŽ¯ Best Evolved Prompt:\")\n",
    "            print(f'   \"{summary[\"best_prompt\"]}\"')\n",
    "else:\n",
    "    print(\"âš ï¸  No results to analyze - experiment was not run or failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance statistics\n",
    "if 'summary' in locals() and 'performance' in summary:\n",
    "    perf = summary['performance']\n",
    "    \n",
    "    print(\"âš¡ Performance Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Runtime: {perf.get('total_runtime_minutes', 0):.1f} minutes\")\n",
    "    \n",
    "    if 'api_usage' in perf:\n",
    "        api = perf['api_usage']\n",
    "        print(f\"\\nðŸ”Œ API Usage:\")\n",
    "        print(f\"   Total API Calls: {api.get('total_calls', 0)}\")\n",
    "        print(f\"   Total Tokens: {api.get('total_tokens', 0):,}\")\n",
    "        print(f\"   Tokens per Call: {api.get('tokens_per_call', 0):.1f}\")\n",
    "    \n",
    "    if 'cache_performance' in perf:\n",
    "        cache = perf['cache_performance']\n",
    "        print(f\"\\nðŸ’¾ Cache Performance:\")\n",
    "        print(f\"   Hit Rate: {cache.get('hit_rate', 0):.1%}\")\n",
    "        print(f\"   Total Hits: {cache.get('total_hits', 0)}\")\n",
    "        print(f\"   Total Misses: {cache.get('total_misses', 0)}\")\n",
    "    \n",
    "    if 'memory_usage' in perf:\n",
    "        memory = perf['memory_usage']\n",
    "        print(f\"\\nðŸ§  Memory Usage:\")\n",
    "        print(f\"   Peak Memory: {memory.get('peak_mb', 0):.1f} MB\")\n",
    "        print(f\"   Memory Growth: {memory.get('growth_mb', 0):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Evolution Progress\n",
    "\n",
    "Let's look at the evolution progress through visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evolution plots if available\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if 'runner' in locals() and runner.experiment_id:\n",
    "    # Look for generated plots\n",
    "    plots_dir = config.get_data_dir() / \"plots\" / runner.experiment_id\n",
    "    \n",
    "    if plots_dir.exists():\n",
    "        print(\"ðŸ“ˆ Evolution Visualizations:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Show final evolution progress plot\n",
    "        final_plot = plots_dir / \"final_evolution_progress.png\"\n",
    "        if final_plot.exists():\n",
    "            print(\"\\nðŸ”¹ Evolution Progress:\")\n",
    "            display(Image(str(final_plot)))\n",
    "        \n",
    "        # Show convergence analysis\n",
    "        convergence_plot = plots_dir / \"convergence_analysis.png\"\n",
    "        if convergence_plot.exists():\n",
    "            print(\"\\nðŸ”¹ Convergence Analysis:\")\n",
    "            display(Image(str(convergence_plot)))\n",
    "        \n",
    "        # List all available plots\n",
    "        all_plots = list(plots_dir.glob(\"*.png\"))\n",
    "        print(f\"\\nðŸ“Š Total plots generated: {len(all_plots)}\")\n",
    "        for plot in all_plots:\n",
    "            print(f\"   - {plot.name}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No visualization plots found.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare with Baseline Prompts\n",
    "\n",
    "Let's compare our evolved prompt with some baseline prompts to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline prompts for comparison\n",
    "baseline_prompts = [\n",
    "    \"Solve this math problem.\",\n",
    "    \"Let's solve this step by step.\",\n",
    "    \"Think carefully and solve this problem.\",\n",
    "    \"Calculate the answer to this question.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“‹ Baseline Prompts for Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "for i, prompt in enumerate(baseline_prompts, 1):\n",
    "    print(f\"{i}. \\\"{prompt}\\\"\")\n",
    "\n",
    "if 'summary' in locals() and summary.get('best_prompt'):\n",
    "    print(f\"\\nðŸ§¬ Evolved Prompt:\")\n",
    "    print(f'   \"{summary[\"best_prompt\"]}\"')\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Best Fitness Achieved: {summary.get('results', {}).get('best_fitness', 0):.3f}\")\n",
    "    print(\"\\nðŸ’¡ The evolved prompt should show improvements in:\")\n",
    "    print(\"   - Mathematical reasoning clarity\")\n",
    "    print(\"   - Step-by-step problem solving\")\n",
    "    print(\"   - Accuracy on GSM8K problems\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No evolved prompt available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Custom Experiment Configurations\n",
    "\n",
    "Here are examples of how to set up different types of experiments for research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ablation Study - No Crossover\n",
    "ablation_config = config_manager.create_custom_config('standard', {\n",
    "    'name': 'Ablation Study: Mutation Only',\n",
    "    'crossover_rate': 0.0,\n",
    "    'mutation_rate': 0.5,\n",
    "    'max_generations': 50\n",
    "})\n",
    "\n",
    "print(\"ðŸ”¬ Ablation Study Configuration:\")\n",
    "print(config_manager.get_config_summary(ablation_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Parameter Sweep - Different Model Comparison\n",
    "model_configs = {\n",
    "    'gpt-3.5-turbo': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0},\n",
    "    'gpt-4': {'model_name': 'gpt-4', 'temperature': 0.0},\n",
    "    'gpt-4-creative': {'model_name': 'gpt-4', 'temperature': 0.3}\n",
    "}\n",
    "\n",
    "print(\"ðŸ”„ Model Comparison Configurations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, modifications in model_configs.items():\n",
    "    config = config_manager.create_custom_config('quick_test', {\n",
    "        'name': f'Model Comparison: {name}',\n",
    "        **modifications\n",
    "    })\n",
    "    print(f\"\\nðŸ”¹ {name}:\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Temperature: {config.temperature}\")\n",
    "    print(f\"   Population: {config.population_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Custom Seed Prompts\n",
    "custom_seeds = [\n",
    "    \"Let me approach this systematically by breaking down the problem.\",\n",
    "    \"I'll solve this by identifying the key information and working step by step.\",\n",
    "    \"To find the answer, I need to carefully analyze what's given and what's asked.\"\n",
    "]\n",
    "\n",
    "custom_seed_config = config_manager.create_custom_config('quick_test', {\n",
    "    'name': 'Custom Seed Experiment',\n",
    "    'custom_seeds': custom_seeds,\n",
    "    'population_size': len(custom_seeds) * 3  # Expand from custom seeds\n",
    "})\n",
    "\n",
    "print(\"ðŸŒ± Custom Seed Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Custom Seeds: {len(custom_seeds)}\")\n",
    "print(f\"Population Size: {custom_seed_config.population_size}\")\n",
    "print(\"\\nCustom Seed Prompts:\")\n",
    "for i, seed in enumerate(custom_seeds, 1):\n",
    "    print(f\"   {i}. \\\"{seed}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment Management and History\n",
    "\n",
    "Learn how to manage multiple experiments and track your research progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "all_experiments = experiment_manager.list_experiments()\n",
    "\n",
    "print(\"ðŸ“š Experiment History:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if all_experiments:\n",
    "    for exp in all_experiments[:5]:  # Show last 5 experiments\n",
    "        print(f\"\\nðŸ”¹ {exp.experiment_name}\")\n",
    "        print(f\"   ID: {exp.experiment_id}\")\n",
    "        print(f\"   Status: {exp.status}\")\n",
    "        print(f\"   Created: {time.ctime(exp.created_at)}\")\n",
    "        if exp.status == 'completed':\n",
    "            print(f\"   Best Fitness: {exp.best_fitness:.3f}\")\n",
    "            print(f\"   Generations: {exp.total_generations}\")\n",
    "            print(f\"   Runtime: {exp.total_time:.1f}s\")\n",
    "else:\n",
    "    print(\"No experiments found in history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment summary statistics\n",
    "summary_stats = experiment_manager.get_experiment_summary()\n",
    "\n",
    "print(\"ðŸ“Š Overall Experiment Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Experiments: {summary_stats['total_experiments']}\")\n",
    "print(f\"Completed: {summary_stats['status_counts'].get('completed', 0)}\")\n",
    "print(f\"Running: {summary_stats['status_counts'].get('running', 0)}\")\n",
    "print(f\"Failed: {summary_stats['status_counts'].get('failed', 0)}\")\n",
    "\n",
    "if summary_stats['completed_experiments'] > 0:\n",
    "    print(f\"\\nðŸ“ˆ Averages (Completed Experiments):\")\n",
    "    print(f\"   Average Best Fitness: {summary_stats['average_best_fitness']:.3f}\")\n",
    "    print(f\"   Average Generations: {summary_stats['average_generations']:.1f}\")\n",
    "    print(f\"   Average Runtime: {summary_stats['average_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Tips and Best Practices\n",
    "\n",
    "Here are some recommendations for getting the best results from your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ **Experiment Design Tips:**\n",
    "\n",
    "1. **Start Small**: Use `quick_test` preset first to validate your setup\n",
    "2. **Problem Count**: More evaluation problems = more accurate fitness but slower evolution\n",
    "3. **Population Size**: Larger populations explore more but cost more API calls\n",
    "4. **Generations**: Allow enough generations for convergence (typically 50-100)\n",
    "\n",
    "### âš™ï¸ **Parameter Tuning:**\n",
    "\n",
    "- **High Exploration**: Increase mutation rate (0.3-0.5) and population size\n",
    "- **High Exploitation**: Increase crossover rate (0.8-0.9) and elite size\n",
    "- **Balanced**: Use default parameters from `standard` preset\n",
    "\n",
    "### ðŸ’° **Cost Management:**\n",
    "\n",
    "- Enable caching (`use_cache=True`) to avoid re-evaluating identical prompts\n",
    "- Start with `gpt-3.5-turbo` before trying more expensive models\n",
    "- Use smaller problem sets for initial experiments\n",
    "\n",
    "### ðŸ“Š **Result Interpretation:**\n",
    "\n",
    "- Fitness > 0.8 indicates strong mathematical reasoning\n",
    "- Look for convergence patterns in the evolution plots\n",
    "- Compare evolved prompts with baseline prompts on held-out test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup and Next Steps\n",
    "\n",
    "Clean up resources and explore further research directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup experiment resources\n",
    "if 'runner' in locals():\n",
    "    runner.cleanup()\n",
    "    print(\"ðŸ§¹ Experiment resources cleaned up\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Tutorial completed successfully!\")\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"   1. Try different experiment presets\")\n",
    "print(\"   2. Experiment with custom seed prompts\")\n",
    "print(\"   3. Compare different models (GPT-3.5, GPT-4, Claude)\")\n",
    "print(\"   4. Run ablation studies to understand component contributions\")\n",
    "print(\"   5. Evaluate evolved prompts on additional math datasets\")\n",
    "\n",
    "print(\"\\nðŸ“š For more advanced usage, see:\")\n",
    "print(\"   - scripts/run_experiment.py for command-line usage\")\n",
    "print(\"   - src/config/experiment_configs.py for configuration options\")\n",
    "print(\"   - Generated plots and logs in the data/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsm8k_ga_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
