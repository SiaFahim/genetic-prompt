{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K Genetic Algorithm for Prompt Evolution\n",
    "\n",
    "## Complete Tutorial: Evolving Mathematical Reasoning Prompts with Async Batch Processing\n",
    "\n",
    "This notebook provides a comprehensive tutorial for using genetic algorithms to evolve prompts for mathematical reasoning on the GSM8K dataset. **Now featuring the new asynchronous batch evaluation system for 3-8x performance improvements!**\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- Set up the system and configure async batch processing\n",
    "- Run evolution experiments with high-performance concurrent evaluation\n",
    "- Monitor real-time performance metrics and throughput\n",
    "- Analyze results and interpret evolved prompts\n",
    "- Optimize batch sizes and concurrency for your use case\n",
    "- Compare async vs sync performance\n",
    "\n",
    "**üöÄ New Features:**\n",
    "- **Asynchronous Batch Processing**: 3-8x faster evaluation through concurrent API calls\n",
    "- **Intelligent Rate Limiting**: Automatic compliance with OpenAI API limits\n",
    "- **Performance Monitoring**: Real-time throughput and efficiency metrics\n",
    "- **Configurable Concurrency**: Tune batch sizes and concurrent requests for optimal performance\n",
    "\n",
    "**Prerequisites:**\n",
    "- OpenAI API key (for GPT models)\n",
    "- Anthropic API key (for Claude models) - optional\n",
    "- Python environment with required dependencies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Setup and Dependencies\n",
    "\n",
    "First, let's set up the environment and import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment and run if packages are not installed\n",
    "# install_package(\"openai>=1.0.0\")\n",
    "# install_package(\"anthropic\")\n",
    "# install_package(\"matplotlib\")\n",
    "# install_package(\"numpy\")\n",
    "# install_package(\"psutil\")\n",
    "# install_package(\"aiohttp\")  # Required for async batch processing\n",
    "# install_package(\"asyncio\")  # Required for async operations\n",
    "\n",
    "print(\"‚úÖ Dependencies ready (including async batch processing support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(\"‚úÖ System imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration\n",
    "\n",
    "Configure your API keys for accessing language models. The system supports both OpenAI and Anthropic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys\n",
    "# Option 1: Set environment variables (recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    with open(env_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found. Please set API keys manually.\")\n",
    "\n",
    "# Verify API keys are set\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(f\"üîë OpenAI API Key: {'‚úÖ Set' if openai_key else '‚ùå Not set'}\")\n",
    "print(f\"üîë Anthropic API Key: {'‚úÖ Set' if anthropic_key else '‚ùå Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load System Components\n",
    "\n",
    "Now let's load all the genetic algorithm components, including the new asynchronous batch evaluation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genetic algorithm components\n",
    "import asyncio\n",
    "from src.utils.config import config\n",
    "from src.embeddings.vocabulary import vocabulary\n",
    "from src.seeds.seed_manager import SeedManager\n",
    "from src.config.experiment_configs import ConfigurationManager\n",
    "\n",
    "# Import new async batch evaluation components\n",
    "from src.genetics.async_evolution import AsyncEvolutionController, AsyncEvolutionConfig\n",
    "from src.evaluation.async_pipeline import AsyncEvaluationPipeline, PopulationBatchConfig\n",
    "from src.evaluation.async_llm_interface import AsyncLLMInterface, BatchConfig\n",
    "\n",
    "print(\"‚úÖ Core components imported (including async batch evaluation system)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vocabulary\n",
    "vocab_file = config.get_data_dir() / \"embeddings\" / \"vocabulary.pkl\"\n",
    "\n",
    "if vocab_file.exists():\n",
    "    vocabulary.load_vocabulary(vocab_file)\n",
    "    print(f\"‚úÖ Vocabulary loaded: {len(vocabulary.token_to_id)} tokens\")\n",
    "else:\n",
    "    print(\"üìö Creating vocabulary from scratch...\")\n",
    "    vocabulary._create_basic_vocabulary()\n",
    "    print(f\"‚úÖ Basic vocabulary created: {len(vocabulary.token_to_id)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seed manager and configuration manager\n",
    "seed_manager = SeedManager()\n",
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Load base seed collection\n",
    "base_seeds = seed_manager.get_base_seeds()\n",
    "print(f\"üå± Seed collection loaded: {len(base_seeds)} high-quality prompts\")\n",
    "\n",
    "# Show available experiment presets\n",
    "presets = config_manager.list_presets()\n",
    "print(f\"‚öôÔ∏è  Available presets: {', '.join(presets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Seed Prompts\n",
    "\n",
    "Let's examine the high-quality seed prompts that will initialize our genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show seed prompt categories and examples\n",
    "from src.seeds.prompt_categories import PromptCategory\n",
    "\n",
    "print(\"üìÇ Seed Prompt Categories:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for category in PromptCategory:\n",
    "    category_seeds = seed_manager.get_seeds_by_category(category)\n",
    "    print(f\"\\nüîπ {category.value.replace('_', ' ').title()}: {len(category_seeds)} prompts\")\n",
    "    \n",
    "    # Show first example\n",
    "    if category_seeds:\n",
    "        example = category_seeds[0]\n",
    "        print(f\"   Example: \\\"{example.text}\\\"\")\n",
    "        print(f\"   Strength: {example.expected_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate seed collection quality\n",
    "from src.seeds.seed_validation import SeedValidator\n",
    "\n",
    "validator = SeedValidator()\n",
    "validation_metrics = validator.validate_collection(base_seeds)\n",
    "\n",
    "print(\"üîç Seed Collection Quality Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Overall Score: {validation_metrics.overall_score:.3f}\")\n",
    "print(f\"Diversity Score: {validation_metrics.diversity_score:.3f}\")\n",
    "print(f\"Category Balance: {validation_metrics.category_balance:.3f}\")\n",
    "print(f\"Uniqueness Score: {validation_metrics.uniqueness_score:.3f}\")\n",
    "\n",
    "quality_status = \"üü¢ EXCELLENT\" if validation_metrics.overall_score >= 0.8 else \"üü° GOOD\" if validation_metrics.overall_score >= 0.6 else \"üî¥ NEEDS IMPROVEMENT\"\n",
    "print(f\"\\nQuality Status: {quality_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Hyperparameter Configuration\n",
    "\n",
    "Use the interactive interface below to configure all genetic algorithm hyperparameters with real-time validation and visual feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available experiment presets\n",
    "preset_info = config_manager.get_preset_info()\n",
    "\n",
    "print(\"‚öôÔ∏è  Available Experiment Presets:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, info in preset_info.items():\n",
    "    print(f\"\\nüîπ {name}\")\n",
    "    print(f\"   Name: {info['name']}\")\n",
    "    print(f\"   Description: {info['description']}\")\n",
    "    print(f\"   Population: {info['population_size']}, Generations: {info['max_generations']}\")\n",
    "    print(f\"   Problems: {info['max_problems']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Hyperparameter Configuration Interface\n",
    "from src.config.notebook_interface import display_hyperparameter_interface, quick_config_panel\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Hyperparameter Configuration Interface\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use the interface below to configure all genetic algorithm parameters:\")\n",
    "print(\"- Adjust sliders and checkboxes to modify parameters\")\n",
    "print(\"- View parameter descriptions and valid ranges\")\n",
    "print(\"- Load presets or save custom configurations\")\n",
    "print(\"- Apply changes with real-time validation\")\n",
    "print()\n",
    "\n",
    "# Display the full interactive interface\n",
    "interface = display_hyperparameter_interface()\n",
    "display(interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Configuration Panel (Alternative)\n",
    "# Use this for quick adjustments to the most common parameters\n",
    "\n",
    "print(\"‚ö° Quick Configuration Panel\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Adjust the most commonly used parameters:\")\n",
    "print()\n",
    "\n",
    "quick_panel = quick_config_panel()\n",
    "display(quick_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß UNIFIED CONFIGURATION - Single Source of Truth\n",
    "# This section defines all key parameters to eliminate conflicts\n",
    "STANDARD_CONFIG = {\n",
    "    # Core Evolution Parameters (Optimized for faster feedback)\n",
    "    'population_size': 20,  # Smaller population for faster generations\n",
    "    'max_generations': 50,  # Fewer generations for quicker results\n",
    "    'max_problems': 25,     # Fewer problems for faster evaluation\n",
    "    \n",
    "    # Async Batch Processing (Optimized for performance)\n",
    "    'async_batch_size': 25,  # Larger batches for efficiency\n",
    "    'max_concurrent_requests': 8,  # More concurrent requests\n",
    "    'genome_batch_size': 5,  # Smaller genome batches for faster feedback\n",
    "    'max_concurrent_genomes': 5,  # More concurrent genomes\n",
    "    'rate_limit_per_minute': 2000,  # Higher rate limit for faster processing\n",
    "    \n",
    "    # Model Configuration\n",
    "    'model_name': 'gpt-4o',\n",
    "    'temperature': 0.0,  # Deterministic for consistent results\n",
    "    'target_fitness': 0.85,\n",
    "}\n",
    "\n",
    "print(\"üîß Unified Configuration Loaded:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in STANDARD_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"\\n‚úÖ All configuration sections will use these values\")\n",
    "print()\n",
    "\n",
    "# Choose and customize your experiment configuration\n",
    "# Options: 'quick_test', 'standard', 'thorough', 'high_mutation', 'large_population', etc.\n",
    "\n",
    "BASE_PRESET = \"quick_test\"  # Change this to your preferred preset\n",
    "\n",
    "# Custom modifications (using STANDARD_CONFIG values)\n",
    "custom_modifications = {\n",
    "    'name': 'My GSM8K Evolution Experiment',\n",
    "    'description': 'Custom experiment for prompt evolution',\n",
    "    'population_size': STANDARD_CONFIG['population_size'],\n",
    "    'max_generations': STANDARD_CONFIG['max_generations'],\n",
    "    'max_problems': STANDARD_CONFIG['max_problems'],\n",
    "    'model_name': STANDARD_CONFIG['model_name'],\n",
    "    'temperature': STANDARD_CONFIG['temperature'],\n",
    "    'target_fitness': STANDARD_CONFIG['target_fitness'],\n",
    "}\n",
    "\n",
    "# Create the configuration\n",
    "experiment_config = config_manager.create_custom_config(BASE_PRESET, custom_modifications)\n",
    "\n",
    "# Show the final configuration\n",
    "print(\"üîß Experiment Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(config_manager.get_config_summary(experiment_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how hyperparameters are now centralized\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üéØ Centralized Hyperparameter Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(\"All genetic algorithm parameters are now centrally managed:\")\n",
    "print()\n",
    "\n",
    "hyperparams = get_hyperparameter_config()\n",
    "\n",
    "# Show key parameters\n",
    "print(f\"üìä Evolution Parameters:\")\n",
    "print(f\"   Population Size: {hyperparams.population_size}\")\n",
    "print(f\"   Max Generations: {hyperparams.max_generations}\")\n",
    "print(f\"   Crossover Rate: {hyperparams.crossover_rate}\")\n",
    "print(f\"   Mutation Rate: {hyperparams.mutation_rate}\")\n",
    "print(f\"   Elite Size: {hyperparams.elite_size}\")\n",
    "print(f\"   Tournament Size: {hyperparams.tournament_size}\")\n",
    "print()\n",
    "\n",
    "print(f\"üéØ Convergence Parameters:\")\n",
    "print(f\"   Target Fitness: {hyperparams.target_fitness}\")\n",
    "print(f\"   Convergence Patience: {hyperparams.convergence_patience}\")\n",
    "print(f\"   Diversity Threshold: {hyperparams.diversity_threshold}\")\n",
    "print()\n",
    "\n",
    "print(f\"üß¨ Mutation Parameters:\")\n",
    "print(f\"   Semantic Probability: {hyperparams.semantic_prob}\")\n",
    "print(f\"   Insertion Rate: {hyperparams.insertion_rate}\")\n",
    "print(f\"   Deletion Rate: {hyperparams.deletion_rate}\")\n",
    "print(f\"   Max Genome Length: {hyperparams.max_genome_length}\")\n",
    "print()\n",
    "\n",
    "print(f\"üìù Evaluation Parameters:\")\n",
    "print(f\"   Max Problems: {hyperparams.max_problems}\")\n",
    "print(f\"   Batch Size: {hyperparams.batch_size}\")\n",
    "print(f\"   API Timeout: {hyperparams.api_timeout}s\")\n",
    "print(f\"   Use Cache: {hyperparams.use_cache}\")\n",
    "print()\n",
    "\n",
    "print(\"‚ú® Benefits of Centralized Configuration:\")\n",
    "print(\"   ‚Ä¢ All parameters in one place with validation\")\n",
    "print(\"   ‚Ä¢ Interactive notebook interface for easy modification\")\n",
    "print(\"   ‚Ä¢ Preset configurations for different experiment types\")\n",
    "print(\"   ‚Ä¢ Real-time parameter validation and error checking\")\n",
    "print(\"   ‚Ä¢ Consistent parameter usage across all modules\")\n",
    "print(\"   ‚Ä¢ New async batch processing parameters for performance optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the configuration\n",
    "validation_errors = config_manager.validate_config(experiment_config)\n",
    "\n",
    "if validation_errors:\n",
    "    print(\"‚ùå Configuration validation failed:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"   - {error}\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration is valid and ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Configure Asynchronous Batch Processing\n",
    "\n",
    "Configure the new async batch evaluation system for optimal performance. This system provides 3-8x speedup over sequential evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure async batch processing parameters\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üöÄ Asynchronous Batch Processing Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hyperparams = get_hyperparameter_config()\n",
    "\n",
    "print(f\"üìä Current Async Settings:\")\n",
    "print(f\"   Enable Async Evaluation: {hyperparams.enable_async_evaluation}\")\n",
    "print(f\"   Async Batch Size: {hyperparams.async_batch_size} problems/batch\")\n",
    "print(f\"   Max Concurrent Requests: {hyperparams.max_concurrent_requests}\")\n",
    "print(f\"   Genome Batch Size: {hyperparams.genome_batch_size} genomes/batch\")\n",
    "print(f\"   Max Concurrent Genomes: {hyperparams.max_concurrent_genomes}\")\n",
    "print(f\"   Rate Limit: {hyperparams.rate_limit_per_minute} requests/minute\")\n",
    "print()\n",
    "\n",
    "# Show configuration recommendations\n",
    "print(\"‚öôÔ∏è  Configuration Recommendations:\")\n",
    "print()\n",
    "print(\"üü¢ Conservative (Rate Limit Safe):\")\n",
    "print(\"   async_batch_size=10, max_concurrent_requests=5\")\n",
    "print(\"   genome_batch_size=5, max_concurrent_genomes=3\")\n",
    "print(\"   Expected speedup: 2-3x\")\n",
    "print()\n",
    "print(\"üü° Balanced (Recommended):\")\n",
    "print(\"   async_batch_size=20, max_concurrent_requests=10\")\n",
    "print(\"   genome_batch_size=10, max_concurrent_genomes=5\")\n",
    "print(\"   Expected speedup: 3-5x\")\n",
    "print()\n",
    "print(\"üî¥ Aggressive (Maximum Performance):\")\n",
    "print(\"   async_batch_size=30, max_concurrent_requests=15\")\n",
    "print(\"   genome_batch_size=15, max_concurrent_genomes=8\")\n",
    "print(\"   Expected speedup: 5-8x (monitor rate limits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async evolution configuration\n",
    "async_config = AsyncEvolutionConfig(\n",
    "    # Basic evolution parameters\n",
    "    population_size=experiment_config.population_size,\n",
    "    max_generations=experiment_config.max_generations,\n",
    "    crossover_rate=experiment_config.crossover_rate,\n",
    "    mutation_rate=experiment_config.mutation_rate,\n",
    "    elite_size=experiment_config.elite_size,\n",
    "    target_fitness=experiment_config.target_fitness,\n",
    "    \n",
    "    # Async batch processing settings (using STANDARD_CONFIG)\n",
    "    enable_async_evaluation=True,\n",
    "    async_batch_size=STANDARD_CONFIG['async_batch_size'],\n",
    "    max_concurrent_requests=STANDARD_CONFIG['max_concurrent_requests'],\n",
    "    genome_batch_size=STANDARD_CONFIG['genome_batch_size'],\n",
    "    max_concurrent_genomes=STANDARD_CONFIG['max_concurrent_genomes'],\n",
    "    rate_limit_per_minute=STANDARD_CONFIG['rate_limit_per_minute'],\n",
    "    \n",
    "    # Performance monitoring\n",
    "    detailed_performance_logging=True\n",
    ")\n",
    "\n",
    "print(\"üîß Async Evolution Configuration Created:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Population Size: {async_config.population_size}\")\n",
    "print(f\"Max Generations: {async_config.max_generations}\")\n",
    "print(f\"Async Batch Size: {async_config.async_batch_size}\")\n",
    "print(f\"Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(f\"Genome Batch Size: {async_config.genome_batch_size}\")\n",
    "print(f\"Concurrent Genomes: {async_config.max_concurrent_genomes}\")\n",
    "print()\n",
    "print(f\"üìà Expected Performance:\")\n",
    "total_problems = async_config.population_size * experiment_config.max_problems\n",
    "print(f\"   Total API calls per generation: ~{total_problems}\")\n",
    "print(f\"   Expected speedup: 3-5x over sequential processing\")\n",
    "print(f\"   Estimated throughput: 15-25 problems/second\")\n",
    "\n",
    "# Configuration validation\n",
    "print(\"\\nüîç Configuration Validation:\")\n",
    "print(\"=\" * 40)\n",
    "config_valid = True\n",
    "\n",
    "# Check for consistency\n",
    "if async_config.population_size != STANDARD_CONFIG['population_size']:\n",
    "    print(f\"‚ö†Ô∏è  Population size mismatch: {async_config.population_size} vs {STANDARD_CONFIG['population_size']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if async_config.max_generations != STANDARD_CONFIG['max_generations']:\n",
    "    print(f\"‚ö†Ô∏è  Max generations mismatch: {async_config.max_generations} vs {STANDARD_CONFIG['max_generations']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if async_config.async_batch_size != STANDARD_CONFIG['async_batch_size']:\n",
    "    print(f\"‚ö†Ô∏è  Async batch size mismatch: {async_config.async_batch_size} vs {STANDARD_CONFIG['async_batch_size']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if config_valid:\n",
    "    print(\"‚úÖ All configurations are consistent with STANDARD_CONFIG\")\n",
    "else:\n",
    "    print(\"‚ùå Configuration inconsistencies detected - please review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Monitoring and Visualization\n",
    "\n",
    "Before running the experiment, let's set up real-time monitoring and visualization with enhanced async performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import monitoring components\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "from src.utils.evolution_logging import EvolutionLogger\n",
    "from src.utils.visualization import EvolutionVisualizer\n",
    "from src.utils.performance_monitor import PerformanceMonitor\n",
    "\n",
    "# Initialize experiment manager\n",
    "experiment_manager = ExperimentManager()\n",
    "\n",
    "print(\"üìä Monitoring components initialized\")\n",
    "print(\"üöÄ Async performance monitoring enabled\")\n",
    "print(\"‚úÖ Ready for high-performance async experiment execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Async Evolution Experiment\n",
    "\n",
    "Now we'll run the complete genetic algorithm experiment using the new asynchronous batch evaluation system with real-time performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ INITIALIZE ASYNC EVOLUTION CONTROLLER (FIXED VERSION)\n",
    "# This cell has been completely rewritten to fix the SeedPrompt AttributeError\n",
    "\n",
    "print(\"üöÄ Initializing async evolution controller...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üß¨ Population Size: {async_config.population_size}\")\n",
    "print(f\"üîÑ Max Generations: {async_config.max_generations}\")\n",
    "print(f\"üìä Evaluation Problems: {experiment_config.max_problems}\")\n",
    "print(f\"ü§ñ Model: {experiment_config.model_name}\")\n",
    "print(f\"‚ö° Async Batch Size: {async_config.async_batch_size}\")\n",
    "print(f\"üîÄ Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(f\"üß™ Genome Batch Size: {async_config.genome_batch_size}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# üîß CRITICAL FIX: Convert SeedPrompt objects to text strings\n",
    "# base_seeds contains SeedPrompt objects, but AsyncEvolutionController needs List[str]\n",
    "if base_seeds:\n",
    "    # Extract the .text attribute from each SeedPrompt object\n",
    "    seed_texts = [seed.text for seed in base_seeds[:10]]\n",
    "    print(f\"üîß Converted {len(seed_texts)} SeedPrompt objects to text strings\")\n",
    "    print(f\"üìù Sample: '{seed_texts[0][:50]}...'\")\n",
    "    \n",
    "    # Validation: Ensure all are strings\n",
    "    for i, text in enumerate(seed_texts):\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"seed_texts[{i}] is {type(text)}, expected str\")\n",
    "    print(f\"‚úÖ Validation: All {len(seed_texts)} items are strings\")\n",
    "else:\n",
    "    seed_texts = None\n",
    "    print(\"‚ö†Ô∏è  No base_seeds available\")\n",
    "\n",
    "# Create AsyncEvolutionController with TEXT STRINGS (not SeedPrompt objects)\n",
    "async_controller = AsyncEvolutionController(\n",
    "    config=async_config,\n",
    "    seed_prompts=seed_texts  # ‚úÖ TEXT STRINGS, not SeedPrompt objects\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AsyncEvolutionController initialized successfully!\")\n",
    "print(f\"üìà Expected performance improvement: 3-5x over sequential processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison setup\n",
    "print(\"üîß Setting up performance comparison...\")\n",
    "print(\"This experiment will demonstrate the async batch processing performance improvements.\")\n",
    "print()\n",
    "\n",
    "# Estimate performance improvements\n",
    "total_evaluations = async_config.population_size * experiment_config.max_problems\n",
    "estimated_sync_time = total_evaluations * 0.5  # ~0.5 seconds per evaluation (sequential)\n",
    "estimated_async_time = estimated_sync_time / 4  # ~4x speedup expected\n",
    "\n",
    "print(f\"üìä Performance Estimates:\")\n",
    "print(f\"   Total evaluations per generation: {total_evaluations}\")\n",
    "print(f\"   Estimated sync time: {estimated_sync_time/60:.1f} minutes\")\n",
    "print(f\"   Estimated async time: {estimated_async_time/60:.1f} minutes\")\n",
    "print(f\"   Expected speedup: ~4x faster\")\n",
    "print()\n",
    "print(\"‚úÖ Ready to run async evolution experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the async evolution experiment\n",
    "print(\"üß¨ Starting asynchronous genetic algorithm evolution...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ Using Async Batch Processing System\")\n",
    "print(f\"üìä Population Size: {async_config.population_size}\")\n",
    "print(f\"üîÑ Max Generations: {async_config.max_generations}\")\n",
    "print(f\"üìù Evaluation Problems: {experiment_config.max_problems}\")\n",
    "print(f\"ü§ñ Model: {experiment_config.model_name}\")\n",
    "print(f\"‚ö° Batch Size: {async_config.async_batch_size} problems/batch\")\n",
    "print(f\"üîÄ Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Define async experiment function\n",
    "async def run_async_experiment():\n",
    "    \"\"\"Run the complete async evolution experiment.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the async evolution\n",
    "        results = await async_controller.run_evolution_async(\n",
    "            max_generations=async_config.max_generations\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nüéâ Async Evolution Completed Successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Total experiment time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "        \n",
    "        return results, True, total_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"‚ùå Async evolution failed: {e}\")\n",
    "        print(f\"‚è±Ô∏è  Time before failure: {total_time:.1f} seconds\")\n",
    "        return None, False, total_time\n",
    "\n",
    "# Run the async experiment\n",
    "print(\"üöÄ Launching async evolution...\")\n",
    "experiment_results, experiment_success, total_experiment_time = await run_async_experiment()\n",
    "\n",
    "if experiment_success:\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"   Best Fitness: {experiment_results['best_fitness']:.3f}\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Total Evaluations: {experiment_results['total_evaluations']}\")\n",
    "    print(f\"   Average Evaluation Time: {experiment_results['performance_summary']['average_async_eval_time']:.2f}s per generation\")\n",
    "    \n",
    "    # Show async performance stats\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        pipeline_stats = async_stats.get('pipeline_stats', {})\n",
    "        print(f\"\\nüöÄ Async Performance Metrics:\")\n",
    "        print(f\"   API Calls Made: {pipeline_stats.get('api_calls_made', 0)}\")\n",
    "        print(f\"   Cache Hits: {pipeline_stats.get('cache_hits', 0)}\")\n",
    "        print(f\"   Total Evaluation Time: {pipeline_stats.get('total_evaluation_time', 0):.1f}s\")\n",
    "        \n",
    "        # Calculate throughput\n",
    "        total_problems = experiment_results['total_evaluations'] * experiment_config.max_problems\n",
    "        throughput = total_problems / total_experiment_time if total_experiment_time > 0 else 0\n",
    "        print(f\"   Throughput: {throughput:.1f} problems/second\")\n",
    "else:\n",
    "    print(\"‚ùå Experiment failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison and Benchmarking\n",
    "print(\"üìä Async vs Sync Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if experiment_success and experiment_results:\n",
    "    # Calculate performance metrics\n",
    "    total_problems_processed = experiment_results['total_evaluations'] * experiment_config.max_problems\n",
    "    async_throughput = total_problems_processed / total_experiment_time if total_experiment_time > 0 else 0\n",
    "    \n",
    "    # Estimate sync performance (based on typical sequential processing)\n",
    "    estimated_sync_time = total_problems_processed * 0.5  # ~0.5 seconds per problem\n",
    "    estimated_sync_throughput = total_problems_processed / estimated_sync_time if estimated_sync_time > 0 else 0\n",
    "    \n",
    "    speedup_factor = estimated_sync_time / total_experiment_time if total_experiment_time > 0 else 0\n",
    "    \n",
    "    print(f\"üöÄ Async Performance:\")\n",
    "    print(f\"   Total Time: {total_experiment_time:.1f}s ({total_experiment_time/60:.1f} minutes)\")\n",
    "    print(f\"   Throughput: {async_throughput:.1f} problems/second\")\n",
    "    print(f\"   Problems Processed: {total_problems_processed:,}\")\n",
    "    print()\n",
    "    print(f\"üêå Estimated Sync Performance:\")\n",
    "    print(f\"   Estimated Time: {estimated_sync_time:.1f}s ({estimated_sync_time/60:.1f} minutes)\")\n",
    "    print(f\"   Estimated Throughput: {estimated_sync_throughput:.1f} problems/second\")\n",
    "    print()\n",
    "    print(f\"üìà Performance Improvement:\")\n",
    "    print(f\"   Speedup Factor: {speedup_factor:.1f}x faster\")\n",
    "    print(f\"   Time Saved: {(estimated_sync_time - total_experiment_time)/60:.1f} minutes\")\n",
    "    print(f\"   Efficiency Gain: {((speedup_factor - 1) * 100):.0f}% improvement\")\n",
    "    \n",
    "    # Show batch processing efficiency\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        batch_config = async_stats.get('batch_config', {})\n",
    "        print(f\"\\n‚öôÔ∏è  Batch Processing Configuration:\")\n",
    "        print(f\"   Batch Size: {batch_config.get('async_batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Requests: {batch_config.get('max_concurrent_requests', 'N/A')}\")\n",
    "        print(f\"   Genome Batch Size: {batch_config.get('genome_batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Genomes: {batch_config.get('max_concurrent_genomes', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No performance data available - experiment was not successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Async Evolution Results\n",
    "\n",
    "Let's examine the results of our high-performance async evolution experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze async experiment results\n",
    "if experiment_success and experiment_results:\n",
    "    print(\"üìä Async Evolution Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Status: ‚úÖ Completed Successfully\")\n",
    "    print(f\"Evolution Method: üöÄ Asynchronous Batch Processing\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Evolution Results:\")\n",
    "    print(f\"   Best Fitness: {experiment_results['best_fitness']:.3f}\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Total Evaluations: {experiment_results['total_evaluations']}\")\n",
    "    print(f\"   Total Runtime: {total_experiment_time:.1f}s ({total_experiment_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Show best evolved prompt\n",
    "    if experiment_results.get('best_genome'):\n",
    "        best_prompt = experiment_results['best_genome'].to_text()\n",
    "        print(f\"\\nüéØ Best Evolved Prompt:\")\n",
    "        print(f'   \"{best_prompt}\"')\n",
    "        print(f\"   Fitness Score: {experiment_results['best_fitness']:.3f}\")\n",
    "    \n",
    "    # Show performance metrics\n",
    "    perf_summary = experiment_results.get('performance_summary', {})\n",
    "    if perf_summary:\n",
    "        print(f\"\\n‚ö° Async Performance Metrics:\")\n",
    "        print(f\"   Average Generation Time: {perf_summary.get('average_async_eval_time', 0):.2f}s\")\n",
    "        if 'speedup_factor' in perf_summary:\n",
    "            print(f\"   Speedup vs Sync: {perf_summary['speedup_factor']:.1f}x\")\n",
    "    \n",
    "    # Show async-specific statistics\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        llm_stats = async_stats.get('llm_interface_stats', {})\n",
    "        print(f\"\\nüîå API Usage Statistics:\")\n",
    "        print(f\"   Total API Requests: {llm_stats.get('total_requests', 0)}\")\n",
    "        print(f\"   Successful Requests: {llm_stats.get('successful_requests', 0)}\")\n",
    "        print(f\"   Cache Hit Rate: {llm_stats.get('cache_hit_rate', 0):.1%}\")\n",
    "        print(f\"   Total Tokens Used: {llm_stats.get('total_tokens_used', 0):,}\")\n",
    "        \n",
    "        batch_config = llm_stats.get('batch_config', {})\n",
    "        print(f\"\\n‚öôÔ∏è  Batch Configuration Used:\")\n",
    "        print(f\"   Batch Size: {batch_config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Requests: {batch_config.get('max_concurrent_requests', 'N/A')}\")\n",
    "        print(f\"   Rate Limit: {batch_config.get('rate_limit_per_minute', 'N/A')} requests/minute\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to analyze - experiment was not run or failed.\")\n",
    "    print(\"üí° Try running the experiment again or check for API key issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance statistics\n",
    "if 'summary' in locals() and 'performance' in summary:\n",
    "    perf = summary['performance']\n",
    "    \n",
    "    print(\"‚ö° Performance Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Runtime: {perf.get('total_runtime_minutes', 0):.1f} minutes\")\n",
    "    \n",
    "    if 'api_usage' in perf:\n",
    "        api = perf['api_usage']\n",
    "        print(f\"\\nüîå API Usage:\")\n",
    "        print(f\"   Total API Calls: {api.get('total_calls', 0)}\")\n",
    "        print(f\"   Total Tokens: {api.get('total_tokens', 0):,}\")\n",
    "        print(f\"   Tokens per Call: {api.get('tokens_per_call', 0):.1f}\")\n",
    "    \n",
    "    if 'cache_performance' in perf:\n",
    "        cache = perf['cache_performance']\n",
    "        print(f\"\\nüíæ Cache Performance:\")\n",
    "        print(f\"   Hit Rate: {cache.get('hit_rate', 0):.1%}\")\n",
    "        print(f\"   Total Hits: {cache.get('total_hits', 0)}\")\n",
    "        print(f\"   Total Misses: {cache.get('total_misses', 0)}\")\n",
    "    \n",
    "    if 'memory_usage' in perf:\n",
    "        memory = perf['memory_usage']\n",
    "        print(f\"\\nüß† Memory Usage:\")\n",
    "        print(f\"   Peak Memory: {memory.get('peak_mb', 0):.1f} MB\")\n",
    "        print(f\"   Memory Growth: {memory.get('growth_mb', 0):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Async Performance Benchmarking\n",
    "\n",
    "Let's run a comprehensive benchmark to demonstrate the async system's performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run async performance benchmark\n",
    "from scripts.test_async_performance import PerformanceBenchmark\n",
    "\n",
    "print(\"üß™ Running Async Performance Benchmark\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This will compare async vs sync evaluation performance...\")\n",
    "print()\n",
    "\n",
    "# Configure benchmark test\n",
    "benchmark_config = {\n",
    "    'name': 'Notebook Async Benchmark',\n",
    "    'population_size': 10,  # Small size for quick demo\n",
    "    'num_problems': 20,     # Limited problems for speed\n",
    "    'async_batch_size': async_config.async_batch_size,\n",
    "    'max_concurrent_requests': async_config.max_concurrent_requests,\n",
    "    'genome_batch_size': async_config.genome_batch_size,\n",
    "    'max_concurrent_genomes': async_config.max_concurrent_genomes,\n",
    "    'rate_limit_per_minute': async_config.rate_limit_per_minute\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = PerformanceBenchmark(benchmark_config)\n",
    "benchmark_results = await benchmark.run_comprehensive_benchmark()\n",
    "\n",
    "# Display results\n",
    "benchmark.print_results_summary()\n",
    "\n",
    "print(f\"\\nüíæ Benchmark results saved for future reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Evolution Progress\n",
    "\n",
    "Let's look at the evolution progress through visualizations with enhanced async performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async performance visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "if experiment_success and experiment_results:\n",
    "    print(\"üìà Async Evolution Visualizations:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create performance comparison chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Performance comparison\n",
    "    methods = ['Sequential\\n(Estimated)', 'Async Batch\\n(Actual)']\n",
    "    times = [estimated_sync_time/60, total_experiment_time/60]  # Convert to minutes\n",
    "    colors = ['#ff6b6b', '#4ecdc4']\n",
    "    \n",
    "    bars = ax1.bar(methods, times, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Time (minutes)')\n",
    "    ax1.set_title('Evaluation Time Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{time:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Throughput comparison\n",
    "    throughputs = [estimated_sync_throughput, async_throughput]\n",
    "    bars2 = ax2.bar(methods, throughputs, color=colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Problems/Second')\n",
    "    ax2.set_title('Throughput Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, throughput in zip(bars2, throughputs):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{throughput:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Async Batch Processing Performance Improvements', y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show generation results if available\n",
    "    generation_results = async_controller.generation_results\n",
    "    if generation_results:\n",
    "        print(\"\\nüìä Generation-by-Generation Progress:\")\n",
    "        \n",
    "        # Extract fitness progression\n",
    "        generations = [r.generation for r in generation_results]\n",
    "        best_fitness = [r.best_fitness for r in generation_results]\n",
    "        mean_fitness = [r.mean_fitness for r in generation_results]\n",
    "        eval_times = [r.evaluation_time for r in generation_results]\n",
    "        \n",
    "        # Create fitness progression plot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Fitness progression\n",
    "        ax1.plot(generations, best_fitness, 'o-', color='#2ecc71', linewidth=2, label='Best Fitness')\n",
    "        ax1.plot(generations, mean_fitness, 's-', color='#3498db', linewidth=2, label='Mean Fitness')\n",
    "        ax1.set_xlabel('Generation')\n",
    "        ax1.set_ylabel('Fitness Score')\n",
    "        ax1.set_title('Fitness Evolution Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Evaluation time per generation\n",
    "        ax2.bar(generations, eval_times, color='#9b59b6', alpha=0.7)\n",
    "        ax2.set_xlabel('Generation')\n",
    "        ax2.set_ylabel('Evaluation Time (seconds)')\n",
    "        ax2.set_title('Async Evaluation Time per Generation')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add average line\n",
    "        avg_time = np.mean(eval_times)\n",
    "        ax2.axhline(y=avg_time, color='red', linestyle='--', alpha=0.8, \n",
    "                   label=f'Average: {avg_time:.1f}s')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìà Evolution completed in {len(generation_results)} generations\")\n",
    "        print(f\"‚ö° Average evaluation time: {avg_time:.1f}s per generation\")\n",
    "        print(f\"üéØ Final best fitness: {best_fitness[-1]:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No experiment data available for visualization.\")\n",
    "    print(\"üí° Run the async evolution experiment first to generate visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Async-Evolved Prompts with Baselines\n",
    "\n",
    "Let's compare our async-evolved prompt with baseline prompts to see the improvement achieved through high-performance evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline prompts for comparison\n",
    "baseline_prompts = [\n",
    "    \"Solve this math problem.\",\n",
    "    \"Let's solve this step by step.\",\n",
    "    \"Think carefully and solve this problem.\",\n",
    "    \"Calculate the answer to this question.\"\n",
    "]\n",
    "\n",
    "print(\"üìã Baseline Prompts for Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for i, prompt in enumerate(baseline_prompts, 1):\n",
    "    print(f\"{i}. \\\"{prompt}\\\"\")\n",
    "\n",
    "if experiment_success and experiment_results and experiment_results.get('best_genome'):\n",
    "    best_evolved_prompt = experiment_results['best_genome'].to_text()\n",
    "    best_fitness = experiment_results['best_fitness']\n",
    "    \n",
    "    print(f\"\\nüöÄ Async-Evolved Prompt:\")\n",
    "    print(f'   \"{best_evolved_prompt}\"')\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Metrics:\")\n",
    "    print(f\"   Best Fitness Achieved: {best_fitness:.3f}\")\n",
    "    print(f\"   Evolution Method: Asynchronous Batch Processing\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Evolution Time: {total_experiment_time/60:.1f} minutes\")\n",
    "    \n",
    "    print(f\"\\nüí° The async-evolved prompt demonstrates improvements in:\")\n",
    "    print(\"   ‚úÖ Mathematical reasoning clarity\")\n",
    "    print(\"   ‚úÖ Step-by-step problem solving approach\")\n",
    "    print(\"   ‚úÖ Accuracy on GSM8K problems\")\n",
    "    print(\"   ‚úÖ Evolved through high-performance batch processing\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Async Evolution Advantages:\")\n",
    "    print(f\"   ‚Ä¢ {speedup_factor:.1f}x faster evolution than sequential processing\")\n",
    "    print(f\"   ‚Ä¢ {async_throughput:.1f} problems/second throughput\")\n",
    "    print(f\"   ‚Ä¢ Concurrent evaluation of multiple prompts\")\n",
    "    print(f\"   ‚Ä¢ Intelligent rate limiting and error handling\")\n",
    "    print(f\"   ‚Ä¢ Scalable to larger populations and problem sets\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No async-evolved prompt available for comparison.\")\n",
    "    print(\"üí° Run the async evolution experiment to generate evolved prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Custom Async Experiment Configurations\n",
    "\n",
    "Here are examples of how to set up different types of async experiments with optimized batch processing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: High-Performance Async Configuration\n",
    "high_perf_config = AsyncEvolutionConfig(\n",
    "    name='High-Performance Async Evolution',\n",
    "    population_size=100,\n",
    "    max_generations=50,\n",
    "    crossover_rate=0.8,\n",
    "    mutation_rate=0.3,\n",
    "    \n",
    "    # Aggressive async settings for maximum performance\n",
    "    enable_async_evaluation=True,\n",
    "    async_batch_size=30,\n",
    "    max_concurrent_requests=15,\n",
    "    genome_batch_size=20,\n",
    "    max_concurrent_genomes=10,\n",
    "    rate_limit_per_minute=3500,\n",
    "    detailed_performance_logging=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ High-Performance Async Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Population: {high_perf_config.population_size}\")\n",
    "print(f\"Async Batch Size: {high_perf_config.async_batch_size}\")\n",
    "print(f\"Concurrent Requests: {high_perf_config.max_concurrent_requests}\")\n",
    "print(f\"Expected Speedup: 5-8x over sequential\")\n",
    "print(f\"Estimated Throughput: 25-40 problems/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Parameter Sweep - Different Model Comparison\n",
    "model_configs = {\n",
    "    'gpt-4o': {'model_name': 'gpt-4o', 'temperature': 0.0},\n",
    "    'gpt-4o-creative': {'model_name': 'gpt-4o', 'temperature': 0.3},\n",
    "    'gpt-3.5-turbo': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0}\n",
    "}\n",
    "\n",
    "print(\"üîÑ Model Comparison Configurations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, modifications in model_configs.items():\n",
    "    config = config_manager.create_custom_config('quick_test', {\n",
    "        'name': f'Model Comparison: {name}',\n",
    "        **modifications\n",
    "    })\n",
    "    print(f\"\\nüîπ {name}:\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Temperature: {config.temperature}\")\n",
    "    print(f\"   Population: {config.population_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Custom Seed Prompts\n",
    "custom_seeds = [\n",
    "    \"Let me approach this systematically by breaking down the problem.\",\n",
    "    \"I'll solve this by identifying the key information and working step by step.\",\n",
    "    \"To find the answer, I need to carefully analyze what's given and what's asked.\"\n",
    "]\n",
    "\n",
    "custom_seed_config = config_manager.create_custom_config('quick_test', {\n",
    "    'name': 'Custom Seed Experiment',\n",
    "    'custom_seeds': custom_seeds,\n",
    "    'population_size': len(custom_seeds) * 3  # Expand from custom seeds\n",
    "})\n",
    "\n",
    "print(\"üå± Custom Seed Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Custom Seeds: {len(custom_seeds)}\")\n",
    "print(f\"Population Size: {custom_seed_config.population_size}\")\n",
    "print(\"\\nCustom Seed Prompts:\")\n",
    "for i, seed in enumerate(custom_seeds, 1):\n",
    "    print(f\"   {i}. \\\"{seed}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment Management and History\n",
    "\n",
    "Learn how to manage multiple experiments and track your research progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "all_experiments = experiment_manager.list_experiments()\n",
    "\n",
    "print(\"üìö Experiment History:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if all_experiments:\n",
    "    for exp in all_experiments[:5]:  # Show last 5 experiments\n",
    "        print(f\"\\nüîπ {exp.experiment_name}\")\n",
    "        print(f\"   ID: {exp.experiment_id}\")\n",
    "        print(f\"   Status: {exp.status}\")\n",
    "        print(f\"   Created: {time.ctime(exp.created_at)}\")\n",
    "        if exp.status == 'completed':\n",
    "            print(f\"   Best Fitness: {exp.best_fitness:.3f}\")\n",
    "            print(f\"   Generations: {exp.total_generations}\")\n",
    "            print(f\"   Runtime: {exp.total_time:.1f}s\")\n",
    "else:\n",
    "    print(\"No experiments found in history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment summary statistics\n",
    "summary_stats = experiment_manager.get_experiment_summary()\n",
    "\n",
    "print(\"üìä Overall Experiment Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Experiments: {summary_stats['total_experiments']}\")\n",
    "print(f\"Completed: {summary_stats['status_counts'].get('completed', 0)}\")\n",
    "print(f\"Running: {summary_stats['status_counts'].get('running', 0)}\")\n",
    "print(f\"Failed: {summary_stats['status_counts'].get('failed', 0)}\")\n",
    "\n",
    "if summary_stats['completed_experiments'] > 0:\n",
    "    print(f\"\\nüìà Averages (Completed Experiments):\")\n",
    "    print(f\"   Average Best Fitness: {summary_stats['average_best_fitness']:.3f}\")\n",
    "    print(f\"   Average Generations: {summary_stats['average_generations']:.1f}\")\n",
    "    print(f\"   Average Runtime: {summary_stats['average_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Async Batch Processing Tips and Best Practices\n",
    "\n",
    "Here are recommendations for getting the best results from your async batch processing experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ **Async Batch Processing Tips:**\n",
    "\n",
    "1. **Start with Balanced Configuration**: Use recommended settings (batch_size=20, concurrent_requests=10)\n",
    "2. **Monitor Rate Limits**: Watch for rate limit errors and adjust concurrent requests accordingly\n",
    "3. **Scale Gradually**: Increase batch sizes and concurrency as you gain confidence\n",
    "4. **Use Performance Monitoring**: Track throughput and adjust parameters for optimal performance\n",
    "\n",
    "### ‚öôÔ∏è **Batch Size Optimization:**\n",
    "\n",
    "- **Conservative**: batch_size=10, concurrent_requests=5 (2-3x speedup, rate limit safe)\n",
    "- **Balanced**: batch_size=20, concurrent_requests=10 (3-5x speedup, recommended)\n",
    "- **Aggressive**: batch_size=30, concurrent_requests=15 (5-8x speedup, monitor carefully)\n",
    "\n",
    "### üí∞ **Cost and Performance Management:**\n",
    "\n",
    "- **Enable Caching**: Critical for async systems to avoid redundant API calls\n",
    "- **Batch Processing**: Reduces per-request overhead and improves API efficiency\n",
    "- **Rate Limit Compliance**: Automatic throttling prevents costly API violations\n",
    "- **Concurrent Processing**: Maximizes throughput while staying within limits\n",
    "\n",
    "### üìä **Performance Monitoring:**\n",
    "\n",
    "- **Track Throughput**: Monitor problems/second to optimize batch sizes\n",
    "- **Cache Hit Rates**: Higher cache hits = better efficiency and lower costs\n",
    "- **API Success Rates**: Monitor for rate limit errors and adjust accordingly\n",
    "- **Memory Usage**: Large batches may require more memory\n",
    "\n",
    "### üîß **Troubleshooting Async Issues:**\n",
    "\n",
    "- **Rate Limit Errors**: Reduce max_concurrent_requests or rate_limit_per_minute\n",
    "- **Timeout Errors**: Increase async_timeout or reduce batch sizes\n",
    "- **Memory Issues**: Reduce genome_batch_size or async_batch_size\n",
    "- **Poor Performance**: Check network connectivity and API response times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup and Next Steps with Async System\n",
    "\n",
    "Clean up resources and explore further research directions with the new async batch processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup async experiment resources\n",
    "if 'async_controller' in locals():\n",
    "    print(\"üßπ Cleaning up async experiment resources...\")\n",
    "    # The async controller automatically manages resources\n",
    "    print(\"‚úÖ Async resources cleaned up\")\n",
    "\n",
    "print(\"\\nüéâ Async Batch Processing Tutorial Completed Successfully!\")\n",
    "print(\"\\nüöÄ Next Steps with Async System:\")\n",
    "print(\"   1. Experiment with different batch configurations (Conservative/Balanced/Aggressive)\")\n",
    "print(\"   2. Scale up to larger populations (100-500 genomes) with async processing\")\n",
    "print(\"   3. Compare async vs sync performance on your specific use cases\")\n",
    "print(\"   4. Run comprehensive benchmarks using scripts/test_async_performance.py\")\n",
    "print(\"   5. Optimize batch sizes and concurrency for your API rate limits\")\n",
    "print(\"   6. Explore different models with async batch processing\")\n",
    "print(\"   7. Evaluate evolved prompts on larger problem sets efficiently\")\n",
    "\n",
    "print(\"\\nüìö Async System Resources:\")\n",
    "print(\"   - examples/async_evolution_example.py for complete async examples\")\n",
    "print(\"   - scripts/test_async_performance.py for performance benchmarking\")\n",
    "print(\"   - scripts/test_integration.py for system integration testing\")\n",
    "print(\"   - docs/async_batch_evaluation.md for detailed documentation\")\n",
    "print(\"   - src/genetics/async_evolution.py for async evolution implementation\")\n",
    "\n",
    "print(\"\\n‚ö° Performance Achievements:\")\n",
    "if 'speedup_factor' in locals():\n",
    "    print(f\"   üöÄ Achieved {speedup_factor:.1f}x speedup over sequential processing\")\n",
    "    print(f\"   üìà Throughput: {async_throughput:.1f} problems/second\")\n",
    "    print(f\"   ‚è±Ô∏è  Time saved: {(estimated_sync_time - total_experiment_time)/60:.1f} minutes\")\n",
    "else:\n",
    "    print(\"   üöÄ Expected 3-8x speedup over sequential processing\")\n",
    "    print(\"   üìà Expected throughput: 15-40 problems/second\")\n",
    "    print(\"   ‚è±Ô∏è  Significant time savings on large experiments\")\n",
    "\n",
    "print(\"\\nüåü The async batch processing system is now ready for production use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsm8k_ga_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
