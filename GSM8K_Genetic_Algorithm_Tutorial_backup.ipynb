{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K Genetic Algorithm for Prompt Evolution\n",
    "\n",
    "## Complete Tutorial: Evolving Mathematical Reasoning Prompts with Async Batch Processing\n",
    "\n",
    "This notebook provides a comprehensive tutorial for using genetic algorithms to evolve prompts for mathematical reasoning on the GSM8K dataset. **Now featuring the new asynchronous batch evaluation system for 3-8x performance improvements!**\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- Set up the system and configure async batch processing\n",
    "- Run evolution experiments with high-performance concurrent evaluation\n",
    "- Monitor real-time performance metrics and throughput\n",
    "- Analyze results and interpret evolved prompts\n",
    "- Optimize batch sizes and concurrency for your use case\n",
    "- Compare async vs sync performance\n",
    "\n",
    "**üöÄ New Features:**\n",
    "- **Asynchronous Batch Processing**: 3-8x faster evaluation through concurrent API calls\n",
    "- **Intelligent Rate Limiting**: Automatic compliance with OpenAI API limits\n",
    "- **Performance Monitoring**: Real-time throughput and efficiency metrics\n",
    "- **Configurable Concurrency**: Tune batch sizes and concurrent requests for optimal performance\n",
    "\n",
    "**Prerequisites:**\n",
    "- OpenAI API key (for GPT models)\n",
    "- Anthropic API key (for Claude models) - optional\n",
    "- Python environment with required dependencies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Setup and Dependencies\n",
    "\n",
    "First, let's set up the environment and import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies ready (including async batch processing support)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment and run if packages are not installed\n",
    "# install_package(\"openai>=1.0.0\")\n",
    "# install_package(\"anthropic\")\n",
    "# install_package(\"matplotlib\")\n",
    "# install_package(\"numpy\")\n",
    "# install_package(\"psutil\")\n",
    "# install_package(\"aiohttp\")  # Required for async batch processing\n",
    "# install_package(\"asyncio\")  # Required for async operations\n",
    "\n",
    "print(\"‚úÖ Dependencies ready (including async batch processing support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project root: /Users/Odyssey/Projects/genetic-prompt\n",
      "‚úÖ System imports ready\n"
     ]
    }
   ],
   "source": [
    "# Import system modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(\"‚úÖ System imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration\n",
    "\n",
    "Configure your API keys for accessing language models. The system supports both OpenAI and Anthropic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from .env file\n",
      "üîë OpenAI API Key: ‚úÖ Set\n",
      "üîë Anthropic API Key: ‚úÖ Set\n"
     ]
    }
   ],
   "source": [
    "# Set up API keys\n",
    "# Option 1: Set environment variables (recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    with open(env_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found. Please set API keys manually.\")\n",
    "\n",
    "# Verify API keys are set\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(f\"üîë OpenAI API Key: {'‚úÖ Set' if openai_key else '‚ùå Not set'}\")\n",
    "print(f\"üîë Anthropic API Key: {'‚úÖ Set' if anthropic_key else '‚ùå Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load System Components\n",
    "\n",
    "Now let's load all the genetic algorithm components, including the new asynchronous batch evaluation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache loaded: 0 evaluation entries, 0 fitness entries\n",
      "‚úÖ Core components imported (including async batch evaluation system)\n"
     ]
    }
   ],
   "source": [
    "# Import genetic algorithm components\n",
    "import asyncio\n",
    "from src.utils.config import config\n",
    "from src.embeddings.vocabulary import vocabulary\n",
    "from src.seeds.seed_manager import SeedManager\n",
    "from src.config.experiment_configs import ConfigurationManager\n",
    "\n",
    "# Import new async batch evaluation components\n",
    "from src.genetics.async_evolution import AsyncEvolutionController, AsyncEvolutionConfig\n",
    "from src.evaluation.async_pipeline import AsyncEvaluationPipeline, PopulationBatchConfig\n",
    "from src.evaluation.async_llm_interface import AsyncLLMInterface, BatchConfig\n",
    "\n",
    "print(\"‚úÖ Core components imported (including async batch evaluation system)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from /Users/Odyssey/Projects/genetic-prompt/data/embeddings/vocabulary.pkl\n",
      "Vocabulary size: 10004\n",
      "‚úÖ Vocabulary loaded: 10004 tokens\n"
     ]
    }
   ],
   "source": [
    "# Initialize vocabulary\n",
    "vocab_file = config.get_data_dir() / \"embeddings\" / \"vocabulary.pkl\"\n",
    "\n",
    "if vocab_file.exists():\n",
    "    vocabulary.load_vocabulary(vocab_file)\n",
    "    print(f\"‚úÖ Vocabulary loaded: {len(vocabulary.token_to_id)} tokens\")\n",
    "else:\n",
    "    print(\"üìö Creating vocabulary from scratch...\")\n",
    "    vocabulary._create_basic_vocabulary()\n",
    "    print(f\"‚úÖ Basic vocabulary created: {len(vocabulary.token_to_id)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded collection 'test_collection' with 10 seeds\n",
      "üìÇ Loaded collection 'test_validation_collection' with 10 seeds\n",
      "üå± Seed collection loaded: 50 high-quality prompts\n",
      "‚öôÔ∏è  Available presets: quick_test, standard, thorough, ablation_no_crossover, ablation_no_mutation, high_mutation, large_population, random_search\n"
     ]
    }
   ],
   "source": [
    "# Initialize seed manager and configuration manager\n",
    "seed_manager = SeedManager()\n",
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Load base seed collection\n",
    "base_seeds = seed_manager.get_base_seeds()\n",
    "print(f\"üå± Seed collection loaded: {len(base_seeds)} high-quality prompts\")\n",
    "\n",
    "# Show available experiment presets\n",
    "presets = config_manager.list_presets()\n",
    "print(f\"‚öôÔ∏è  Available presets: {', '.join(presets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Seed Prompts\n",
    "\n",
    "Let's examine the high-quality seed prompts that will initialize our genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Seed Prompt Categories:\n",
      "========================================\n",
      "\n",
      "üîπ Step By Step: 8 prompts\n",
      "   Example: \"Let's solve this step by step.\"\n",
      "   Strength: Clear sequential reasoning\n",
      "\n",
      "üîπ Visual Reasoning: 4 prompts\n",
      "   Example: \"Let me visualize this problem to better understand it.\"\n",
      "   Strength: Better spatial understanding\n",
      "\n",
      "üîπ Algebraic Approach: 5 prompts\n",
      "   Example: \"Let me define variables and set up equations for this problem.\"\n",
      "   Strength: Handles unknown quantities\n",
      "\n",
      "üîπ Logical Breakdown: 5 prompts\n",
      "   Example: \"Let me think about this logically and reason through each part.\"\n",
      "   Strength: Clear reasoning chains\n",
      "\n",
      "üîπ Pattern Recognition: 4 prompts\n",
      "   Example: \"I notice a pattern here that can help solve this more efficiently.\"\n",
      "   Strength: Efficient solutions\n",
      "\n",
      "üîπ Estimation Checking: 5 prompts\n",
      "   Example: \"Let me estimate the answer first, then calculate precisely.\"\n",
      "   Strength: Error detection through estimation\n",
      "\n",
      "üîπ Word Problem Parsing: 6 prompts\n",
      "   Example: \"Let me carefully read and understand what this problem is asking.\"\n",
      "   Strength: Better comprehension\n",
      "\n",
      "üîπ Multiple Methods: 4 prompts\n",
      "   Example: \"I can solve this in several ways - let me choose the most efficient.\"\n",
      "   Strength: Optimal approach choice\n",
      "\n",
      "üîπ Conceptual Understanding: 5 prompts\n",
      "   Example: \"Let me understand the underlying mathematical concept first.\"\n",
      "   Strength: Deep understanding\n",
      "\n",
      "üîπ Systematic Organization: 4 prompts\n",
      "   Example: \"I'll organize all the information systematically before solving.\"\n",
      "   Strength: Reduced confusion\n"
     ]
    }
   ],
   "source": [
    "# Show seed prompt categories and examples\n",
    "from src.seeds.prompt_categories import PromptCategory\n",
    "\n",
    "print(\"üìÇ Seed Prompt Categories:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for category in PromptCategory:\n",
    "    category_seeds = seed_manager.get_seeds_by_category(category)\n",
    "    print(f\"\\nüîπ {category.value.replace('_', ' ').title()}: {len(category_seeds)} prompts\")\n",
    "    \n",
    "    # Show first example\n",
    "    if category_seeds:\n",
    "        example = category_seeds[0]\n",
    "        print(f\"   Example: \\\"{example.text}\\\"\")\n",
    "        print(f\"   Strength: {example.expected_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Seed Collection Quality Report:\n",
      "========================================\n",
      "Overall Score: 0.898\n",
      "Diversity Score: 0.771\n",
      "Category Balance: 1.000\n",
      "Uniqueness Score: 0.912\n",
      "\n",
      "Quality Status: üü¢ EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "# Validate seed collection quality\n",
    "from src.seeds.seed_validation import SeedValidator\n",
    "\n",
    "validator = SeedValidator()\n",
    "validation_metrics = validator.validate_collection(base_seeds)\n",
    "\n",
    "print(\"üîç Seed Collection Quality Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Overall Score: {validation_metrics.overall_score:.3f}\")\n",
    "print(f\"Diversity Score: {validation_metrics.diversity_score:.3f}\")\n",
    "print(f\"Category Balance: {validation_metrics.category_balance:.3f}\")\n",
    "print(f\"Uniqueness Score: {validation_metrics.uniqueness_score:.3f}\")\n",
    "\n",
    "quality_status = \"üü¢ EXCELLENT\" if validation_metrics.overall_score >= 0.8 else \"üü° GOOD\" if validation_metrics.overall_score >= 0.6 else \"üî¥ NEEDS IMPROVEMENT\"\n",
    "print(f\"\\nQuality Status: {quality_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Hyperparameter Configuration\n",
    "\n",
    "Use the interactive interface below to configure all genetic algorithm hyperparameters with real-time validation and visual feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Available Experiment Presets:\n",
      "==================================================\n",
      "\n",
      "üîπ quick_test\n",
      "   Name: Quick Test\n",
      "   Description: Fast test run for system validation\n",
      "   Population: 10, Generations: 15\n",
      "   Problems: 20\n",
      "\n",
      "üîπ standard\n",
      "   Name: Standard Evolution\n",
      "   Description: Standard GSM8K evolution experiment\n",
      "   Population: 50, Generations: 100\n",
      "   Problems: 100\n",
      "\n",
      "üîπ thorough\n",
      "   Name: Thorough Evolution\n",
      "   Description: Comprehensive evolution with large population\n",
      "   Population: 100, Generations: 200\n",
      "   Problems: 200\n",
      "\n",
      "üîπ ablation_no_crossover\n",
      "   Name: Ablation: No Crossover\n",
      "   Description: Evolution with mutation only (no crossover)\n",
      "   Population: 50, Generations: 100\n",
      "   Problems: 100\n",
      "\n",
      "üîπ ablation_no_mutation\n",
      "   Name: Ablation: No Mutation\n",
      "   Description: Evolution with crossover only (no mutation)\n",
      "   Population: 50, Generations: 100\n",
      "   Problems: 100\n",
      "\n",
      "üîπ high_mutation\n",
      "   Name: High Mutation Rate\n",
      "   Description: Evolution with high mutation rate\n",
      "   Population: 50, Generations: 100\n",
      "   Problems: 100\n",
      "\n",
      "üîπ large_population\n",
      "   Name: Large Population\n",
      "   Description: Evolution with large population size\n",
      "   Population: 150, Generations: 50\n",
      "   Problems: 100\n",
      "\n",
      "üîπ random_search\n",
      "   Name: Random Search Baseline\n",
      "   Description: Random search baseline for comparison\n",
      "   Population: 50, Generations: 100\n",
      "   Problems: 100\n"
     ]
    }
   ],
   "source": [
    "# Show available experiment presets\n",
    "preset_info = config_manager.get_preset_info()\n",
    "\n",
    "print(\"‚öôÔ∏è  Available Experiment Presets:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, info in preset_info.items():\n",
    "    print(f\"\\nüîπ {name}\")\n",
    "    print(f\"   Name: {info['name']}\")\n",
    "    print(f\"   Description: {info['description']}\")\n",
    "    print(f\"   Population: {info['population_size']}, Generations: {info['max_generations']}\")\n",
    "    print(f\"   Problems: {info['max_problems']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è Interactive Hyperparameter Configuration Interface\n",
      "============================================================\n",
      "Use the interface below to configure all genetic algorithm parameters:\n",
      "- Adjust sliders and checkboxes to modify parameters\n",
      "- View parameter descriptions and valid ranges\n",
      "- Load presets or save custom configurations\n",
      "- Apply changes with real-time validation\n",
      "\n",
      "Creating hyperparameter interface...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b0f1adcf8749379f1441013c67461b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>üß¨ Simple Hyperparameter Interface</h2>'), IntSlider(value=50, description='Popu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive Hyperparameter Configuration Interface\n",
    "from src.config.notebook_interface import display_hyperparameter_interface, quick_config_panel\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Hyperparameter Configuration Interface\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use the interface below to configure all genetic algorithm parameters:\")\n",
    "print(\"- Adjust sliders and checkboxes to modify parameters\")\n",
    "print(\"- View parameter descriptions and valid ranges\")\n",
    "print(\"- Load presets or save custom configurations\")\n",
    "print(\"- Apply changes with real-time validation\")\n",
    "print()\n",
    "\n",
    "# Display the full interactive interface\n",
    "interface = display_hyperparameter_interface()\n",
    "display(interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Quick Configuration Panel\n",
      "========================================\n",
      "Adjust the most commonly used parameters:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fe6ef87a684458ae83718f40c0c807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>‚ö° Quick Configuration Panel</h3>'), IntSlider(value=50, description='population‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick Configuration Panel (Alternative)\n",
    "# Use this for quick adjustments to the most common parameters\n",
    "\n",
    "print(\"‚ö° Quick Configuration Panel\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Adjust the most commonly used parameters:\")\n",
    "print()\n",
    "\n",
    "quick_panel = quick_config_panel()\n",
    "display(quick_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Unified Configuration Loaded:\n",
      "========================================\n",
      "   population_size: 50\n",
      "   max_generations: 100\n",
      "   max_problems: 100\n",
      "   async_batch_size: 20\n",
      "   max_concurrent_requests: 5\n",
      "   genome_batch_size: 10\n",
      "   max_concurrent_genomes: 3\n",
      "   rate_limit_per_minute: 1000\n",
      "   model_name: gpt-4o\n",
      "   temperature: 0.0\n",
      "   target_fitness: 0.85\n",
      "\n",
      "‚úÖ All configuration sections will use these values\n",
      "\n",
      "üîß Experiment Configuration:\n",
      "========================================\n",
      "üìã My GSM8K Evolution Experiment\n",
      "   Custom experiment for prompt evolution\n",
      "   Type: quick_test\n",
      "   Population: 50\n",
      "   Generations: 100\n",
      "   Problems: 100\n",
      "   Crossover: 80.0%\n",
      "   Mutation: 20.0%\n",
      "   Selection: tournament\n",
      "   Model: gpt-4o\n",
      "   Target Fitness: 0.85\n"
     ]
    }
   ],
   "source": [
    "# üîß UNIFIED CONFIGURATION - Single Source of Truth\n",
    "# This section defines all key parameters to eliminate conflicts\n",
    "STANDARD_CONFIG = {\n",
    "    # Core Evolution Parameters\n",
    "    'population_size': 50,\n",
    "    'max_generations': 100,\n",
    "    'max_problems': 100,\n",
    "    \n",
    "    # Async Batch Processing (Conservative for stability)\n",
    "    'async_batch_size': 20,\n",
    "    'max_concurrent_requests': 5,  # Reduced for rate limit safety\n",
    "    'genome_batch_size': 10,\n",
    "    'max_concurrent_genomes': 3,\n",
    "    'rate_limit_per_minute': 1000,  # Conservative rate limiting\n",
    "    \n",
    "    # Model Configuration\n",
    "    'model_name': 'gpt-4o',\n",
    "    'temperature': 0.0,  # Deterministic for consistent results\n",
    "    'target_fitness': 0.85,\n",
    "}\n",
    "\n",
    "print(\"üîß Unified Configuration Loaded:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in STANDARD_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"\\n‚úÖ All configuration sections will use these values\")\n",
    "print()\n",
    "\n",
    "# Choose and customize your experiment configuration\n",
    "# Options: 'quick_test', 'standard', 'thorough', 'high_mutation', 'large_population', etc.\n",
    "\n",
    "BASE_PRESET = \"quick_test\"  # Change this to your preferred preset\n",
    "\n",
    "# Custom modifications (using STANDARD_CONFIG values)\n",
    "custom_modifications = {\n",
    "    'name': 'My GSM8K Evolution Experiment',\n",
    "    'description': 'Custom experiment for prompt evolution',\n",
    "    'population_size': STANDARD_CONFIG['population_size'],\n",
    "    'max_generations': STANDARD_CONFIG['max_generations'],\n",
    "    'max_problems': STANDARD_CONFIG['max_problems'],\n",
    "    'model_name': STANDARD_CONFIG['model_name'],\n",
    "    'temperature': STANDARD_CONFIG['temperature'],\n",
    "    'target_fitness': STANDARD_CONFIG['target_fitness'],\n",
    "}\n",
    "\n",
    "# Create the configuration\n",
    "experiment_config = config_manager.create_custom_config(BASE_PRESET, custom_modifications)\n",
    "\n",
    "# Show the final configuration\n",
    "print(\"üîß Experiment Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(config_manager.get_config_summary(experiment_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Centralized Hyperparameter Configuration\n",
      "==================================================\n",
      "All genetic algorithm parameters are now centrally managed:\n",
      "\n",
      "üìä Evolution Parameters:\n",
      "   Population Size: 50\n",
      "   Max Generations: 100\n",
      "   Crossover Rate: 0.8\n",
      "   Mutation Rate: 0.2\n",
      "   Elite Size: 5\n",
      "   Tournament Size: 3\n",
      "\n",
      "üéØ Convergence Parameters:\n",
      "   Target Fitness: 0.85\n",
      "   Convergence Patience: 20\n",
      "   Diversity Threshold: 0.05\n",
      "\n",
      "üß¨ Mutation Parameters:\n",
      "   Semantic Probability: 0.9\n",
      "   Insertion Rate: 0.05\n",
      "   Deletion Rate: 0.05\n",
      "   Max Genome Length: 50\n",
      "\n",
      "üìù Evaluation Parameters:\n",
      "   Max Problems: 100\n",
      "   Batch Size: 10\n",
      "   API Timeout: 30s\n",
      "   Use Cache: True\n",
      "\n",
      "‚ú® Benefits of Centralized Configuration:\n",
      "   ‚Ä¢ All parameters in one place with validation\n",
      "   ‚Ä¢ Interactive notebook interface for easy modification\n",
      "   ‚Ä¢ Preset configurations for different experiment types\n",
      "   ‚Ä¢ Real-time parameter validation and error checking\n",
      "   ‚Ä¢ Consistent parameter usage across all modules\n",
      "   ‚Ä¢ New async batch processing parameters for performance optimization\n"
     ]
    }
   ],
   "source": [
    "# Show how hyperparameters are now centralized\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üéØ Centralized Hyperparameter Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(\"All genetic algorithm parameters are now centrally managed:\")\n",
    "print()\n",
    "\n",
    "hyperparams = get_hyperparameter_config()\n",
    "\n",
    "# Show key parameters\n",
    "print(f\"üìä Evolution Parameters:\")\n",
    "print(f\"   Population Size: {hyperparams.population_size}\")\n",
    "print(f\"   Max Generations: {hyperparams.max_generations}\")\n",
    "print(f\"   Crossover Rate: {hyperparams.crossover_rate}\")\n",
    "print(f\"   Mutation Rate: {hyperparams.mutation_rate}\")\n",
    "print(f\"   Elite Size: {hyperparams.elite_size}\")\n",
    "print(f\"   Tournament Size: {hyperparams.tournament_size}\")\n",
    "print()\n",
    "\n",
    "print(f\"üéØ Convergence Parameters:\")\n",
    "print(f\"   Target Fitness: {hyperparams.target_fitness}\")\n",
    "print(f\"   Convergence Patience: {hyperparams.convergence_patience}\")\n",
    "print(f\"   Diversity Threshold: {hyperparams.diversity_threshold}\")\n",
    "print()\n",
    "\n",
    "print(f\"üß¨ Mutation Parameters:\")\n",
    "print(f\"   Semantic Probability: {hyperparams.semantic_prob}\")\n",
    "print(f\"   Insertion Rate: {hyperparams.insertion_rate}\")\n",
    "print(f\"   Deletion Rate: {hyperparams.deletion_rate}\")\n",
    "print(f\"   Max Genome Length: {hyperparams.max_genome_length}\")\n",
    "print()\n",
    "\n",
    "print(f\"üìù Evaluation Parameters:\")\n",
    "print(f\"   Max Problems: {hyperparams.max_problems}\")\n",
    "print(f\"   Batch Size: {hyperparams.batch_size}\")\n",
    "print(f\"   API Timeout: {hyperparams.api_timeout}s\")\n",
    "print(f\"   Use Cache: {hyperparams.use_cache}\")\n",
    "print()\n",
    "\n",
    "print(\"‚ú® Benefits of Centralized Configuration:\")\n",
    "print(\"   ‚Ä¢ All parameters in one place with validation\")\n",
    "print(\"   ‚Ä¢ Interactive notebook interface for easy modification\")\n",
    "print(\"   ‚Ä¢ Preset configurations for different experiment types\")\n",
    "print(\"   ‚Ä¢ Real-time parameter validation and error checking\")\n",
    "print(\"   ‚Ä¢ Consistent parameter usage across all modules\")\n",
    "print(\"   ‚Ä¢ New async batch processing parameters for performance optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration is valid and ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Validate the configuration\n",
    "validation_errors = config_manager.validate_config(experiment_config)\n",
    "\n",
    "if validation_errors:\n",
    "    print(\"‚ùå Configuration validation failed:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"   - {error}\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration is valid and ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Configure Asynchronous Batch Processing\n",
    "\n",
    "Configure the new async batch evaluation system for optimal performance. This system provides 3-8x speedup over sequential evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Asynchronous Batch Processing Configuration\n",
      "============================================================\n",
      "üìä Current Async Settings:\n",
      "   Enable Async Evaluation: True\n",
      "   Async Batch Size: 20 problems/batch\n",
      "   Max Concurrent Requests: 10\n",
      "   Genome Batch Size: 10 genomes/batch\n",
      "   Max Concurrent Genomes: 5\n",
      "   Rate Limit: 3500 requests/minute\n",
      "\n",
      "‚öôÔ∏è  Configuration Recommendations:\n",
      "\n",
      "üü¢ Conservative (Rate Limit Safe):\n",
      "   async_batch_size=10, max_concurrent_requests=5\n",
      "   genome_batch_size=5, max_concurrent_genomes=3\n",
      "   Expected speedup: 2-3x\n",
      "\n",
      "üü° Balanced (Recommended):\n",
      "   async_batch_size=20, max_concurrent_requests=10\n",
      "   genome_batch_size=10, max_concurrent_genomes=5\n",
      "   Expected speedup: 3-5x\n",
      "\n",
      "üî¥ Aggressive (Maximum Performance):\n",
      "   async_batch_size=30, max_concurrent_requests=15\n",
      "   genome_batch_size=15, max_concurrent_genomes=8\n",
      "   Expected speedup: 5-8x (monitor rate limits)\n"
     ]
    }
   ],
   "source": [
    "# Configure async batch processing parameters\n",
    "from src.config.hyperparameters import get_hyperparameter_config\n",
    "\n",
    "print(\"üöÄ Asynchronous Batch Processing Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hyperparams = get_hyperparameter_config()\n",
    "\n",
    "print(f\"üìä Current Async Settings:\")\n",
    "print(f\"   Enable Async Evaluation: {hyperparams.enable_async_evaluation}\")\n",
    "print(f\"   Async Batch Size: {hyperparams.async_batch_size} problems/batch\")\n",
    "print(f\"   Max Concurrent Requests: {hyperparams.max_concurrent_requests}\")\n",
    "print(f\"   Genome Batch Size: {hyperparams.genome_batch_size} genomes/batch\")\n",
    "print(f\"   Max Concurrent Genomes: {hyperparams.max_concurrent_genomes}\")\n",
    "print(f\"   Rate Limit: {hyperparams.rate_limit_per_minute} requests/minute\")\n",
    "print()\n",
    "\n",
    "# Show configuration recommendations\n",
    "print(\"‚öôÔ∏è  Configuration Recommendations:\")\n",
    "print()\n",
    "print(\"üü¢ Conservative (Rate Limit Safe):\")\n",
    "print(\"   async_batch_size=10, max_concurrent_requests=5\")\n",
    "print(\"   genome_batch_size=5, max_concurrent_genomes=3\")\n",
    "print(\"   Expected speedup: 2-3x\")\n",
    "print()\n",
    "print(\"üü° Balanced (Recommended):\")\n",
    "print(\"   async_batch_size=20, max_concurrent_requests=10\")\n",
    "print(\"   genome_batch_size=10, max_concurrent_genomes=5\")\n",
    "print(\"   Expected speedup: 3-5x\")\n",
    "print()\n",
    "print(\"üî¥ Aggressive (Maximum Performance):\")\n",
    "print(\"   async_batch_size=30, max_concurrent_requests=15\")\n",
    "print(\"   genome_batch_size=15, max_concurrent_genomes=8\")\n",
    "print(\"   Expected speedup: 5-8x (monitor rate limits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Async Evolution Configuration Created:\n",
      "==================================================\n",
      "Population Size: 50\n",
      "Max Generations: 100\n",
      "Async Batch Size: 20\n",
      "Concurrent Requests: 5\n",
      "Genome Batch Size: 10\n",
      "Concurrent Genomes: 3\n",
      "\n",
      "üìà Expected Performance:\n",
      "   Total API calls per generation: ~5000\n",
      "   Expected speedup: 3-5x over sequential processing\n",
      "   Estimated throughput: 15-25 problems/second\n",
      "\n",
      "üîç Configuration Validation:\n",
      "========================================\n",
      "‚úÖ All configurations are consistent with STANDARD_CONFIG\n"
     ]
    }
   ],
   "source": [
    "# Create async evolution configuration\n",
    "async_config = AsyncEvolutionConfig(\n",
    "    # Basic evolution parameters\n",
    "    population_size=experiment_config.population_size,\n",
    "    max_generations=experiment_config.max_generations,\n",
    "    crossover_rate=experiment_config.crossover_rate,\n",
    "    mutation_rate=experiment_config.mutation_rate,\n",
    "    elite_size=experiment_config.elite_size,\n",
    "    target_fitness=experiment_config.target_fitness,\n",
    "    \n",
    "    # Async batch processing settings (using STANDARD_CONFIG)\n",
    "    enable_async_evaluation=True,\n",
    "    async_batch_size=STANDARD_CONFIG['async_batch_size'],\n",
    "    max_concurrent_requests=STANDARD_CONFIG['max_concurrent_requests'],\n",
    "    genome_batch_size=STANDARD_CONFIG['genome_batch_size'],\n",
    "    max_concurrent_genomes=STANDARD_CONFIG['max_concurrent_genomes'],\n",
    "    rate_limit_per_minute=STANDARD_CONFIG['rate_limit_per_minute'],\n",
    "    \n",
    "    # Performance monitoring\n",
    "    detailed_performance_logging=True\n",
    ")\n",
    "\n",
    "print(\"üîß Async Evolution Configuration Created:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Population Size: {async_config.population_size}\")\n",
    "print(f\"Max Generations: {async_config.max_generations}\")\n",
    "print(f\"Async Batch Size: {async_config.async_batch_size}\")\n",
    "print(f\"Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(f\"Genome Batch Size: {async_config.genome_batch_size}\")\n",
    "print(f\"Concurrent Genomes: {async_config.max_concurrent_genomes}\")\n",
    "print()\n",
    "print(f\"üìà Expected Performance:\")\n",
    "total_problems = async_config.population_size * experiment_config.max_problems\n",
    "print(f\"   Total API calls per generation: ~{total_problems}\")\n",
    "print(f\"   Expected speedup: 3-5x over sequential processing\")\n",
    "print(f\"   Estimated throughput: 15-25 problems/second\")\n",
    "\n",
    "# Configuration validation\n",
    "print(\"\\nüîç Configuration Validation:\")\n",
    "print(\"=\" * 40)\n",
    "config_valid = True\n",
    "\n",
    "# Check for consistency\n",
    "if async_config.population_size != STANDARD_CONFIG['population_size']:\n",
    "    print(f\"‚ö†Ô∏è  Population size mismatch: {async_config.population_size} vs {STANDARD_CONFIG['population_size']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if async_config.max_generations != STANDARD_CONFIG['max_generations']:\n",
    "    print(f\"‚ö†Ô∏è  Max generations mismatch: {async_config.max_generations} vs {STANDARD_CONFIG['max_generations']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if async_config.async_batch_size != STANDARD_CONFIG['async_batch_size']:\n",
    "    print(f\"‚ö†Ô∏è  Async batch size mismatch: {async_config.async_batch_size} vs {STANDARD_CONFIG['async_batch_size']}\")\n",
    "    config_valid = False\n",
    "\n",
    "if config_valid:\n",
    "    print(\"‚úÖ All configurations are consistent with STANDARD_CONFIG\")\n",
    "else:\n",
    "    print(\"‚ùå Configuration inconsistencies detected - please review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Monitoring and Visualization\n",
    "\n",
    "Before running the experiment, let's set up real-time monitoring and visualization with enhanced async performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Monitoring components initialized\n",
      "üöÄ Async performance monitoring enabled\n",
      "‚úÖ Ready for high-performance async experiment execution\n"
     ]
    }
   ],
   "source": [
    "# Import monitoring components\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "from src.utils.evolution_logging import EvolutionLogger\n",
    "from src.utils.visualization import EvolutionVisualizer\n",
    "from src.utils.performance_monitor import PerformanceMonitor\n",
    "\n",
    "# Initialize experiment manager\n",
    "experiment_manager = ExperimentManager()\n",
    "\n",
    "print(\"üìä Monitoring components initialized\")\n",
    "print(\"üöÄ Async performance monitoring enabled\")\n",
    "print(\"‚úÖ Ready for high-performance async experiment execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Async Evolution Experiment\n",
    "\n",
    "Now we'll run the complete genetic algorithm experiment using the new asynchronous batch evaluation system with real-time performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing async evolution controller...\n",
      "============================================================\n",
      "üß¨ Population Size: 50\n",
      "üîÑ Max Generations: 100\n",
      "üìä Evaluation Problems: 100\n",
      "ü§ñ Model: gpt-4o\n",
      "‚ö° Async Batch Size: 20\n",
      "üîÄ Concurrent Requests: 5\n",
      "üß™ Genome Batch Size: 10\n",
      "============================================================\n",
      "üîß Converted 10 SeedPrompt objects to text strings\n",
      "üìù Sample seed text: 'Let's solve this step by step....'\n",
      "‚úÖ Validation passed: All 10 seed_texts are strings\n",
      "Initialized population with 10 seeds and 40 random genomes\n",
      "Initialized population with 10 seed prompts\n",
      "‚úÖ Population initialized with 50 genomes\n",
      "‚úÖ Async evolution controller initialized successfully!\n",
      "üìà Expected performance improvement: 3-5x over sequential processing\n"
     ]
    }
   ],
   "source": [
    "# Initialize the async evolution controller\n",
    "from dataclasses import asdict\n",
    "\n",
    "print(\"üöÄ Initializing async evolution controller...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üß¨ Population Size: {async_config.population_size}\")\n",
    "print(f\"üîÑ Max Generations: {async_config.max_generations}\")\n",
    "print(f\"üìä Evaluation Problems: {experiment_config.max_problems}\")\n",
    "print(f\"ü§ñ Model: {experiment_config.model_name}\")\n",
    "print(f\"‚ö° Async Batch Size: {async_config.async_batch_size}\")\n",
    "print(f\"üîÄ Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(f\"üß™ Genome Batch Size: {async_config.genome_batch_size}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create async evolution controller\n",
    "# Convert SeedPrompt objects to text strings (CRITICAL FIX)\n",
    "if base_seeds:\n",
    "    seed_texts = [seed.text for seed in base_seeds[:10]]\n",
    "    print(f\"üîß Converted {len(seed_texts)} SeedPrompt objects to text strings\")\n",
    "    print(f\"üìù Sample seed text: '{seed_texts[0][:50]}...'\")\n",
    "else:\n",
    "    seed_texts = None\n",
    "    print(\"‚ö†Ô∏è  No base_seeds available, using None\")\n",
    "\n",
    "# Validation: Ensure we're passing strings, not SeedPrompt objects\n",
    "if seed_texts:\n",
    "    for i, text in enumerate(seed_texts[:3]):\n",
    "        if hasattr(text, 'text'):  # This would indicate a SeedPrompt object\n",
    "            raise TypeError(f\"ERROR: seed_texts[{i}] is still a SeedPrompt object, not a string!\")\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"ERROR: seed_texts[{i}] is {type(text)}, expected str!\")\n",
    "    print(f\"‚úÖ Validation passed: All {len(seed_texts)} seed_texts are strings\")\n",
    "\n",
    "async_controller = AsyncEvolutionController(\n",
    "    config=async_config,\n",
    "    seed_prompts=seed_texts  # Use converted text strings, NOT SeedPrompt objects\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Async evolution controller initialized successfully!\")\n",
    "print(f\"üìà Expected performance improvement: 3-5x over sequential processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up performance comparison...\n",
      "This experiment will demonstrate the async batch processing performance improvements.\n",
      "\n",
      "üìä Performance Estimates:\n",
      "   Total evaluations per generation: 5000\n",
      "   Estimated sync time: 41.7 minutes\n",
      "   Estimated async time: 10.4 minutes\n",
      "   Expected speedup: ~4x faster\n",
      "\n",
      "‚úÖ Ready to run async evolution experiment\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison setup\n",
    "print(\"üîß Setting up performance comparison...\")\n",
    "print(\"This experiment will demonstrate the async batch processing performance improvements.\")\n",
    "print()\n",
    "\n",
    "# Estimate performance improvements\n",
    "total_evaluations = async_config.population_size * experiment_config.max_problems\n",
    "estimated_sync_time = total_evaluations * 0.5  # ~0.5 seconds per evaluation (sequential)\n",
    "estimated_async_time = estimated_sync_time / 4  # ~4x speedup expected\n",
    "\n",
    "print(f\"üìä Performance Estimates:\")\n",
    "print(f\"   Total evaluations per generation: {total_evaluations}\")\n",
    "print(f\"   Estimated sync time: {estimated_sync_time/60:.1f} minutes\")\n",
    "print(f\"   Estimated async time: {estimated_async_time/60:.1f} minutes\")\n",
    "print(f\"   Expected speedup: ~4x faster\")\n",
    "print()\n",
    "print(\"‚úÖ Ready to run async evolution experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Starting asynchronous genetic algorithm evolution...\n",
      "======================================================================\n",
      "üöÄ Using Async Batch Processing System\n",
      "üìä Population Size: 50\n",
      "üîÑ Max Generations: 100\n",
      "üìù Evaluation Problems: 100\n",
      "ü§ñ Model: gpt-4o\n",
      "‚ö° Batch Size: 20 problems/batch\n",
      "üîÄ Concurrent Requests: 5\n",
      "======================================================================\n",
      "\n",
      "üöÄ Launching async evolution...\n",
      "üß¨ Starting async evolution with 50 genomes for 100 generations\n",
      "üìä Async config: batch_size=20, concurrent_requests=5, genome_batch_size=10\n",
      "üìä Gen 0: 50 problems\n",
      "üß¨ Evaluating 50 genomes in 5 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50 problems in 3 batches of size 20\n",
      "Processing 50 problems in 3 batches of size 20\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [02:10<?, ?it/s, genome=seed_0, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 130.34s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [02:26<?, ?it/s, genome=seed_1, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 146.73s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [02:41<?, ?it/s, genome=seed_2, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 161.94s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [04:54<?, ?it/s, genome=seed_0, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 163.77s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [05:11<?, ?it/s, genome=seed_1, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 164.44s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [05:25<?, ?it/s, genome=seed_2, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 163.26s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [06:13<?, ?it/s, genome=seed_0, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 79.60s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [06:25<?, ?it/s, genome=seed_1, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 74.67s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [06:38<?, ?it/s, genome=seed_2, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 73.73s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [08:56<?, ?it/s, genome=seed_3, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 162.86s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [09:09<?, ?it/s, genome=seed_4, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 163.21s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [09:17<?, ?it/s, genome=seed_5, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 159.05s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [11:40<?, ?it/s, genome=seed_3, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 164.32s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [11:53<?, ?it/s, genome=seed_4, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 164.46s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [12:05<?, ?it/s, genome=seed_5, problem=40/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 completed in 167.07s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [12:52<?, ?it/s, genome=seed_3, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 71.70s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [13:06<?, ?it/s, genome=seed_4, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 72.91s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [13:20<?, ?it/s, genome=seed_5, problem=50/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 completed in 75.75s (10 problems)\n",
      "Processing 50 problems in 3 batches of size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [15:42<?, ?it/s, genome=seed_6, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 169.59s (20 problems)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating population:   0%|          | 0/50 [15:55<?, ?it/s, genome=seed_7, problem=20/50, correct=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 completed in 169.50s (20 problems)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Run the async experiment\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Launching async evolution...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m experiment_results, experiment_success, total_experiment_time = \u001b[38;5;28;01mawait\u001b[39;00m run_async_experiment()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m experiment_success:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìà Performance Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_async_experiment\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m start_time = time.time()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Run the async evolution\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m async_controller.run_evolution_async(\n\u001b[32m     22\u001b[39m         max_generations=async_config.max_generations\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m     total_time = time.time() - start_time\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Async Evolution Completed Successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genetic-prompt/src/genetics/async_evolution.py:239\u001b[39m, in \u001b[36mAsyncEvolutionController.run_evolution_async\u001b[39m\u001b[34m(self, max_generations)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Evolution loop\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_gens):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evolve_generation_async()\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGen \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.generation\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: best=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.best_fitness\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.mean_fitness\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | div=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.diversity\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    244\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.evaluation_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genetic-prompt/src/genetics/async_evolution.py:123\u001b[39m, in \u001b[36mAsyncEvolutionController.evolve_generation_async\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m eval_start = time.time()\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_config.enable_async_evaluation:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     population_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_evaluation_pipeline.evaluate_adaptive_async(\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mself\u001b[39m.population, \u001b[38;5;28mself\u001b[39m.population.generation\n\u001b[32m    125\u001b[39m     )\n\u001b[32m    126\u001b[39m     evaluation_results = population_result.evaluation_results\n\u001b[32m    127\u001b[39m     eval_time = population_result.total_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genetic-prompt/src/evaluation/async_pipeline.py:295\u001b[39m, in \u001b[36mAsyncEvaluationPipeline.evaluate_adaptive_async\u001b[39m\u001b[34m(self, population, generation)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.population_batch_config.detailed_logging:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Gen \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(problems)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m problems\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluate_population_async(population, problems)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genetic-prompt/src/evaluation/async_pipeline.py:235\u001b[39m, in \u001b[36mAsyncEvaluationPipeline.evaluate_population_async\u001b[39m\u001b[34m(self, population, problems, progress_callback)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Process genome batch concurrently\u001b[39;00m\n\u001b[32m    234\u001b[39m tasks = [evaluate_genome_with_semaphore(genome) \u001b[38;5;28;01mfor\u001b[39;00m genome \u001b[38;5;129;01min\u001b[39;00m genome_batch]\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m batch_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Process results and handle exceptions\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m genome, (result, error) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(genome_batch, batch_results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genetic-prompt/src/evaluation/async_pipeline.py:226\u001b[39m, in \u001b[36mAsyncEvaluationPipeline.evaluate_population_async.<locals>.evaluate_genome_with_semaphore\u001b[39m\u001b[34m(genome)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_genome_with_semaphore\u001b[39m(genome):\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    228\u001b[39m             result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluate_genome_async(genome, problems, default_progress)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/locks.py:14\u001b[39m, in \u001b[36m_ContextManagerMixin.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.acquire()\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# We have no use for the \"as ...\"  clause in the with\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# statement for locks.\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/locks.py:407\u001b[39m, in \u001b[36mSemaphore.acquire\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    409\u001b[39m         \u001b[38;5;28mself\u001b[39m._waiters.remove(fut)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the async evolution experiment\n",
    "print(\"üß¨ Starting asynchronous genetic algorithm evolution...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ Using Async Batch Processing System\")\n",
    "print(f\"üìä Population Size: {async_config.population_size}\")\n",
    "print(f\"üîÑ Max Generations: {async_config.max_generations}\")\n",
    "print(f\"üìù Evaluation Problems: {experiment_config.max_problems}\")\n",
    "print(f\"ü§ñ Model: {experiment_config.model_name}\")\n",
    "print(f\"‚ö° Batch Size: {async_config.async_batch_size} problems/batch\")\n",
    "print(f\"üîÄ Concurrent Requests: {async_config.max_concurrent_requests}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Define async experiment function\n",
    "async def run_async_experiment():\n",
    "    \"\"\"Run the complete async evolution experiment.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the async evolution\n",
    "        results = await async_controller.run_evolution_async(\n",
    "            max_generations=async_config.max_generations\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nüéâ Async Evolution Completed Successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Total experiment time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "        \n",
    "        return results, True, total_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"‚ùå Async evolution failed: {e}\")\n",
    "        print(f\"‚è±Ô∏è  Time before failure: {total_time:.1f} seconds\")\n",
    "        return None, False, total_time\n",
    "\n",
    "# Run the async experiment\n",
    "print(\"üöÄ Launching async evolution...\")\n",
    "experiment_results, experiment_success, total_experiment_time = await run_async_experiment()\n",
    "\n",
    "if experiment_success:\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"   Best Fitness: {experiment_results['best_fitness']:.3f}\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Total Evaluations: {experiment_results['total_evaluations']}\")\n",
    "    print(f\"   Average Evaluation Time: {experiment_results['performance_summary']['average_async_eval_time']:.2f}s per generation\")\n",
    "    \n",
    "    # Show async performance stats\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        pipeline_stats = async_stats.get('pipeline_stats', {})\n",
    "        print(f\"\\nüöÄ Async Performance Metrics:\")\n",
    "        print(f\"   API Calls Made: {pipeline_stats.get('api_calls_made', 0)}\")\n",
    "        print(f\"   Cache Hits: {pipeline_stats.get('cache_hits', 0)}\")\n",
    "        print(f\"   Total Evaluation Time: {pipeline_stats.get('total_evaluation_time', 0):.1f}s\")\n",
    "        \n",
    "        # Calculate throughput\n",
    "        total_problems = experiment_results['total_evaluations'] * experiment_config.max_problems\n",
    "        throughput = total_problems / total_experiment_time if total_experiment_time > 0 else 0\n",
    "        print(f\"   Throughput: {throughput:.1f} problems/second\")\n",
    "else:\n",
    "    print(\"‚ùå Experiment failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison and Benchmarking\n",
    "print(\"üìä Async vs Sync Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if experiment_success and experiment_results:\n",
    "    # Calculate performance metrics\n",
    "    total_problems_processed = experiment_results['total_evaluations'] * experiment_config.max_problems\n",
    "    async_throughput = total_problems_processed / total_experiment_time if total_experiment_time > 0 else 0\n",
    "    \n",
    "    # Estimate sync performance (based on typical sequential processing)\n",
    "    estimated_sync_time = total_problems_processed * 0.5  # ~0.5 seconds per problem\n",
    "    estimated_sync_throughput = total_problems_processed / estimated_sync_time if estimated_sync_time > 0 else 0\n",
    "    \n",
    "    speedup_factor = estimated_sync_time / total_experiment_time if total_experiment_time > 0 else 0\n",
    "    \n",
    "    print(f\"üöÄ Async Performance:\")\n",
    "    print(f\"   Total Time: {total_experiment_time:.1f}s ({total_experiment_time/60:.1f} minutes)\")\n",
    "    print(f\"   Throughput: {async_throughput:.1f} problems/second\")\n",
    "    print(f\"   Problems Processed: {total_problems_processed:,}\")\n",
    "    print()\n",
    "    print(f\"üêå Estimated Sync Performance:\")\n",
    "    print(f\"   Estimated Time: {estimated_sync_time:.1f}s ({estimated_sync_time/60:.1f} minutes)\")\n",
    "    print(f\"   Estimated Throughput: {estimated_sync_throughput:.1f} problems/second\")\n",
    "    print()\n",
    "    print(f\"üìà Performance Improvement:\")\n",
    "    print(f\"   Speedup Factor: {speedup_factor:.1f}x faster\")\n",
    "    print(f\"   Time Saved: {(estimated_sync_time - total_experiment_time)/60:.1f} minutes\")\n",
    "    print(f\"   Efficiency Gain: {((speedup_factor - 1) * 100):.0f}% improvement\")\n",
    "    \n",
    "    # Show batch processing efficiency\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        batch_config = async_stats.get('batch_config', {})\n",
    "        print(f\"\\n‚öôÔ∏è  Batch Processing Configuration:\")\n",
    "        print(f\"   Batch Size: {batch_config.get('async_batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Requests: {batch_config.get('max_concurrent_requests', 'N/A')}\")\n",
    "        print(f\"   Genome Batch Size: {batch_config.get('genome_batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Genomes: {batch_config.get('max_concurrent_genomes', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No performance data available - experiment was not successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Async Evolution Results\n",
    "\n",
    "Let's examine the results of our high-performance async evolution experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze async experiment results\n",
    "if experiment_success and experiment_results:\n",
    "    print(\"üìä Async Evolution Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Status: ‚úÖ Completed Successfully\")\n",
    "    print(f\"Evolution Method: üöÄ Asynchronous Batch Processing\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Evolution Results:\")\n",
    "    print(f\"   Best Fitness: {experiment_results['best_fitness']:.3f}\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Total Evaluations: {experiment_results['total_evaluations']}\")\n",
    "    print(f\"   Total Runtime: {total_experiment_time:.1f}s ({total_experiment_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Show best evolved prompt\n",
    "    if experiment_results.get('best_genome'):\n",
    "        best_prompt = experiment_results['best_genome'].to_text()\n",
    "        print(f\"\\nüéØ Best Evolved Prompt:\")\n",
    "        print(f'   \"{best_prompt}\"')\n",
    "        print(f\"   Fitness Score: {experiment_results['best_fitness']:.3f}\")\n",
    "    \n",
    "    # Show performance metrics\n",
    "    perf_summary = experiment_results.get('performance_summary', {})\n",
    "    if perf_summary:\n",
    "        print(f\"\\n‚ö° Async Performance Metrics:\")\n",
    "        print(f\"   Average Generation Time: {perf_summary.get('average_async_eval_time', 0):.2f}s\")\n",
    "        if 'speedup_factor' in perf_summary:\n",
    "            print(f\"   Speedup vs Sync: {perf_summary['speedup_factor']:.1f}x\")\n",
    "    \n",
    "    # Show async-specific statistics\n",
    "    async_stats = experiment_results.get('async_stats', {})\n",
    "    if async_stats:\n",
    "        llm_stats = async_stats.get('llm_interface_stats', {})\n",
    "        print(f\"\\nüîå API Usage Statistics:\")\n",
    "        print(f\"   Total API Requests: {llm_stats.get('total_requests', 0)}\")\n",
    "        print(f\"   Successful Requests: {llm_stats.get('successful_requests', 0)}\")\n",
    "        print(f\"   Cache Hit Rate: {llm_stats.get('cache_hit_rate', 0):.1%}\")\n",
    "        print(f\"   Total Tokens Used: {llm_stats.get('total_tokens_used', 0):,}\")\n",
    "        \n",
    "        batch_config = llm_stats.get('batch_config', {})\n",
    "        print(f\"\\n‚öôÔ∏è  Batch Configuration Used:\")\n",
    "        print(f\"   Batch Size: {batch_config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Max Concurrent Requests: {batch_config.get('max_concurrent_requests', 'N/A')}\")\n",
    "        print(f\"   Rate Limit: {batch_config.get('rate_limit_per_minute', 'N/A')} requests/minute\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to analyze - experiment was not run or failed.\")\n",
    "    print(\"üí° Try running the experiment again or check for API key issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance statistics\n",
    "if 'summary' in locals() and 'performance' in summary:\n",
    "    perf = summary['performance']\n",
    "    \n",
    "    print(\"‚ö° Performance Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Runtime: {perf.get('total_runtime_minutes', 0):.1f} minutes\")\n",
    "    \n",
    "    if 'api_usage' in perf:\n",
    "        api = perf['api_usage']\n",
    "        print(f\"\\nüîå API Usage:\")\n",
    "        print(f\"   Total API Calls: {api.get('total_calls', 0)}\")\n",
    "        print(f\"   Total Tokens: {api.get('total_tokens', 0):,}\")\n",
    "        print(f\"   Tokens per Call: {api.get('tokens_per_call', 0):.1f}\")\n",
    "    \n",
    "    if 'cache_performance' in perf:\n",
    "        cache = perf['cache_performance']\n",
    "        print(f\"\\nüíæ Cache Performance:\")\n",
    "        print(f\"   Hit Rate: {cache.get('hit_rate', 0):.1%}\")\n",
    "        print(f\"   Total Hits: {cache.get('total_hits', 0)}\")\n",
    "        print(f\"   Total Misses: {cache.get('total_misses', 0)}\")\n",
    "    \n",
    "    if 'memory_usage' in perf:\n",
    "        memory = perf['memory_usage']\n",
    "        print(f\"\\nüß† Memory Usage:\")\n",
    "        print(f\"   Peak Memory: {memory.get('peak_mb', 0):.1f} MB\")\n",
    "        print(f\"   Memory Growth: {memory.get('growth_mb', 0):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Async Performance Benchmarking\n",
    "\n",
    "Let's run a comprehensive benchmark to demonstrate the async system's performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run async performance benchmark\n",
    "from scripts.test_async_performance import PerformanceBenchmark\n",
    "\n",
    "print(\"üß™ Running Async Performance Benchmark\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This will compare async vs sync evaluation performance...\")\n",
    "print()\n",
    "\n",
    "# Configure benchmark test\n",
    "benchmark_config = {\n",
    "    'name': 'Notebook Async Benchmark',\n",
    "    'population_size': 10,  # Small size for quick demo\n",
    "    'num_problems': 20,     # Limited problems for speed\n",
    "    'async_batch_size': async_config.async_batch_size,\n",
    "    'max_concurrent_requests': async_config.max_concurrent_requests,\n",
    "    'genome_batch_size': async_config.genome_batch_size,\n",
    "    'max_concurrent_genomes': async_config.max_concurrent_genomes,\n",
    "    'rate_limit_per_minute': async_config.rate_limit_per_minute\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = PerformanceBenchmark(benchmark_config)\n",
    "benchmark_results = await benchmark.run_comprehensive_benchmark()\n",
    "\n",
    "# Display results\n",
    "benchmark.print_results_summary()\n",
    "\n",
    "print(f\"\\nüíæ Benchmark results saved for future reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Evolution Progress\n",
    "\n",
    "Let's look at the evolution progress through visualizations with enhanced async performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async performance visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "if experiment_success and experiment_results:\n",
    "    print(\"üìà Async Evolution Visualizations:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create performance comparison chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Performance comparison\n",
    "    methods = ['Sequential\\n(Estimated)', 'Async Batch\\n(Actual)']\n",
    "    times = [estimated_sync_time/60, total_experiment_time/60]  # Convert to minutes\n",
    "    colors = ['#ff6b6b', '#4ecdc4']\n",
    "    \n",
    "    bars = ax1.bar(methods, times, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Time (minutes)')\n",
    "    ax1.set_title('Evaluation Time Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{time:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Throughput comparison\n",
    "    throughputs = [estimated_sync_throughput, async_throughput]\n",
    "    bars2 = ax2.bar(methods, throughputs, color=colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Problems/Second')\n",
    "    ax2.set_title('Throughput Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, throughput in zip(bars2, throughputs):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{throughput:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Async Batch Processing Performance Improvements', y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show generation results if available\n",
    "    generation_results = async_controller.generation_results\n",
    "    if generation_results:\n",
    "        print(\"\\nüìä Generation-by-Generation Progress:\")\n",
    "        \n",
    "        # Extract fitness progression\n",
    "        generations = [r.generation for r in generation_results]\n",
    "        best_fitness = [r.best_fitness for r in generation_results]\n",
    "        mean_fitness = [r.mean_fitness for r in generation_results]\n",
    "        eval_times = [r.evaluation_time for r in generation_results]\n",
    "        \n",
    "        # Create fitness progression plot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Fitness progression\n",
    "        ax1.plot(generations, best_fitness, 'o-', color='#2ecc71', linewidth=2, label='Best Fitness')\n",
    "        ax1.plot(generations, mean_fitness, 's-', color='#3498db', linewidth=2, label='Mean Fitness')\n",
    "        ax1.set_xlabel('Generation')\n",
    "        ax1.set_ylabel('Fitness Score')\n",
    "        ax1.set_title('Fitness Evolution Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Evaluation time per generation\n",
    "        ax2.bar(generations, eval_times, color='#9b59b6', alpha=0.7)\n",
    "        ax2.set_xlabel('Generation')\n",
    "        ax2.set_ylabel('Evaluation Time (seconds)')\n",
    "        ax2.set_title('Async Evaluation Time per Generation')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add average line\n",
    "        avg_time = np.mean(eval_times)\n",
    "        ax2.axhline(y=avg_time, color='red', linestyle='--', alpha=0.8, \n",
    "                   label=f'Average: {avg_time:.1f}s')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìà Evolution completed in {len(generation_results)} generations\")\n",
    "        print(f\"‚ö° Average evaluation time: {avg_time:.1f}s per generation\")\n",
    "        print(f\"üéØ Final best fitness: {best_fitness[-1]:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No experiment data available for visualization.\")\n",
    "    print(\"üí° Run the async evolution experiment first to generate visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Async-Evolved Prompts with Baselines\n",
    "\n",
    "Let's compare our async-evolved prompt with baseline prompts to see the improvement achieved through high-performance evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline prompts for comparison\n",
    "baseline_prompts = [\n",
    "    \"Solve this math problem.\",\n",
    "    \"Let's solve this step by step.\",\n",
    "    \"Think carefully and solve this problem.\",\n",
    "    \"Calculate the answer to this question.\"\n",
    "]\n",
    "\n",
    "print(\"üìã Baseline Prompts for Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for i, prompt in enumerate(baseline_prompts, 1):\n",
    "    print(f\"{i}. \\\"{prompt}\\\"\")\n",
    "\n",
    "if experiment_success and experiment_results and experiment_results.get('best_genome'):\n",
    "    best_evolved_prompt = experiment_results['best_genome'].to_text()\n",
    "    best_fitness = experiment_results['best_fitness']\n",
    "    \n",
    "    print(f\"\\nüöÄ Async-Evolved Prompt:\")\n",
    "    print(f'   \"{best_evolved_prompt}\"')\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Metrics:\")\n",
    "    print(f\"   Best Fitness Achieved: {best_fitness:.3f}\")\n",
    "    print(f\"   Evolution Method: Asynchronous Batch Processing\")\n",
    "    print(f\"   Total Generations: {experiment_results['total_generations']}\")\n",
    "    print(f\"   Evolution Time: {total_experiment_time/60:.1f} minutes\")\n",
    "    \n",
    "    print(f\"\\nüí° The async-evolved prompt demonstrates improvements in:\")\n",
    "    print(\"   ‚úÖ Mathematical reasoning clarity\")\n",
    "    print(\"   ‚úÖ Step-by-step problem solving approach\")\n",
    "    print(\"   ‚úÖ Accuracy on GSM8K problems\")\n",
    "    print(\"   ‚úÖ Evolved through high-performance batch processing\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Async Evolution Advantages:\")\n",
    "    print(f\"   ‚Ä¢ {speedup_factor:.1f}x faster evolution than sequential processing\")\n",
    "    print(f\"   ‚Ä¢ {async_throughput:.1f} problems/second throughput\")\n",
    "    print(f\"   ‚Ä¢ Concurrent evaluation of multiple prompts\")\n",
    "    print(f\"   ‚Ä¢ Intelligent rate limiting and error handling\")\n",
    "    print(f\"   ‚Ä¢ Scalable to larger populations and problem sets\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No async-evolved prompt available for comparison.\")\n",
    "    print(\"üí° Run the async evolution experiment to generate evolved prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Custom Async Experiment Configurations\n",
    "\n",
    "Here are examples of how to set up different types of async experiments with optimized batch processing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: High-Performance Async Configuration\n",
    "high_perf_config = AsyncEvolutionConfig(\n",
    "    name='High-Performance Async Evolution',\n",
    "    population_size=100,\n",
    "    max_generations=50,\n",
    "    crossover_rate=0.8,\n",
    "    mutation_rate=0.3,\n",
    "    \n",
    "    # Aggressive async settings for maximum performance\n",
    "    enable_async_evaluation=True,\n",
    "    async_batch_size=30,\n",
    "    max_concurrent_requests=15,\n",
    "    genome_batch_size=20,\n",
    "    max_concurrent_genomes=10,\n",
    "    rate_limit_per_minute=3500,\n",
    "    detailed_performance_logging=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ High-Performance Async Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Population: {high_perf_config.population_size}\")\n",
    "print(f\"Async Batch Size: {high_perf_config.async_batch_size}\")\n",
    "print(f\"Concurrent Requests: {high_perf_config.max_concurrent_requests}\")\n",
    "print(f\"Expected Speedup: 5-8x over sequential\")\n",
    "print(f\"Estimated Throughput: 25-40 problems/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Parameter Sweep - Different Model Comparison\n",
    "model_configs = {\n",
    "    'gpt-4o': {'model_name': 'gpt-4o', 'temperature': 0.0},\n",
    "    'gpt-4o-creative': {'model_name': 'gpt-4o', 'temperature': 0.3},\n",
    "    'gpt-3.5-turbo': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0}\n",
    "}\n",
    "\n",
    "print(\"üîÑ Model Comparison Configurations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, modifications in model_configs.items():\n",
    "    config = config_manager.create_custom_config('quick_test', {\n",
    "        'name': f'Model Comparison: {name}',\n",
    "        **modifications\n",
    "    })\n",
    "    print(f\"\\nüîπ {name}:\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Temperature: {config.temperature}\")\n",
    "    print(f\"   Population: {config.population_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Custom Seed Prompts\n",
    "custom_seeds = [\n",
    "    \"Let me approach this systematically by breaking down the problem.\",\n",
    "    \"I'll solve this by identifying the key information and working step by step.\",\n",
    "    \"To find the answer, I need to carefully analyze what's given and what's asked.\"\n",
    "]\n",
    "\n",
    "custom_seed_config = config_manager.create_custom_config('quick_test', {\n",
    "    'name': 'Custom Seed Experiment',\n",
    "    'custom_seeds': custom_seeds,\n",
    "    'population_size': len(custom_seeds) * 3  # Expand from custom seeds\n",
    "})\n",
    "\n",
    "print(\"üå± Custom Seed Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Custom Seeds: {len(custom_seeds)}\")\n",
    "print(f\"Population Size: {custom_seed_config.population_size}\")\n",
    "print(\"\\nCustom Seed Prompts:\")\n",
    "for i, seed in enumerate(custom_seeds, 1):\n",
    "    print(f\"   {i}. \\\"{seed}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment Management and History\n",
    "\n",
    "Learn how to manage multiple experiments and track your research progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "all_experiments = experiment_manager.list_experiments()\n",
    "\n",
    "print(\"üìö Experiment History:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if all_experiments:\n",
    "    for exp in all_experiments[:5]:  # Show last 5 experiments\n",
    "        print(f\"\\nüîπ {exp.experiment_name}\")\n",
    "        print(f\"   ID: {exp.experiment_id}\")\n",
    "        print(f\"   Status: {exp.status}\")\n",
    "        print(f\"   Created: {time.ctime(exp.created_at)}\")\n",
    "        if exp.status == 'completed':\n",
    "            print(f\"   Best Fitness: {exp.best_fitness:.3f}\")\n",
    "            print(f\"   Generations: {exp.total_generations}\")\n",
    "            print(f\"   Runtime: {exp.total_time:.1f}s\")\n",
    "else:\n",
    "    print(\"No experiments found in history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment summary statistics\n",
    "summary_stats = experiment_manager.get_experiment_summary()\n",
    "\n",
    "print(\"üìä Overall Experiment Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Experiments: {summary_stats['total_experiments']}\")\n",
    "print(f\"Completed: {summary_stats['status_counts'].get('completed', 0)}\")\n",
    "print(f\"Running: {summary_stats['status_counts'].get('running', 0)}\")\n",
    "print(f\"Failed: {summary_stats['status_counts'].get('failed', 0)}\")\n",
    "\n",
    "if summary_stats['completed_experiments'] > 0:\n",
    "    print(f\"\\nüìà Averages (Completed Experiments):\")\n",
    "    print(f\"   Average Best Fitness: {summary_stats['average_best_fitness']:.3f}\")\n",
    "    print(f\"   Average Generations: {summary_stats['average_generations']:.1f}\")\n",
    "    print(f\"   Average Runtime: {summary_stats['average_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Async Batch Processing Tips and Best Practices\n",
    "\n",
    "Here are recommendations for getting the best results from your async batch processing experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ **Async Batch Processing Tips:**\n",
    "\n",
    "1. **Start with Balanced Configuration**: Use recommended settings (batch_size=20, concurrent_requests=10)\n",
    "2. **Monitor Rate Limits**: Watch for rate limit errors and adjust concurrent requests accordingly\n",
    "3. **Scale Gradually**: Increase batch sizes and concurrency as you gain confidence\n",
    "4. **Use Performance Monitoring**: Track throughput and adjust parameters for optimal performance\n",
    "\n",
    "### ‚öôÔ∏è **Batch Size Optimization:**\n",
    "\n",
    "- **Conservative**: batch_size=10, concurrent_requests=5 (2-3x speedup, rate limit safe)\n",
    "- **Balanced**: batch_size=20, concurrent_requests=10 (3-5x speedup, recommended)\n",
    "- **Aggressive**: batch_size=30, concurrent_requests=15 (5-8x speedup, monitor carefully)\n",
    "\n",
    "### üí∞ **Cost and Performance Management:**\n",
    "\n",
    "- **Enable Caching**: Critical for async systems to avoid redundant API calls\n",
    "- **Batch Processing**: Reduces per-request overhead and improves API efficiency\n",
    "- **Rate Limit Compliance**: Automatic throttling prevents costly API violations\n",
    "- **Concurrent Processing**: Maximizes throughput while staying within limits\n",
    "\n",
    "### üìä **Performance Monitoring:**\n",
    "\n",
    "- **Track Throughput**: Monitor problems/second to optimize batch sizes\n",
    "- **Cache Hit Rates**: Higher cache hits = better efficiency and lower costs\n",
    "- **API Success Rates**: Monitor for rate limit errors and adjust accordingly\n",
    "- **Memory Usage**: Large batches may require more memory\n",
    "\n",
    "### üîß **Troubleshooting Async Issues:**\n",
    "\n",
    "- **Rate Limit Errors**: Reduce max_concurrent_requests or rate_limit_per_minute\n",
    "- **Timeout Errors**: Increase async_timeout or reduce batch sizes\n",
    "- **Memory Issues**: Reduce genome_batch_size or async_batch_size\n",
    "- **Poor Performance**: Check network connectivity and API response times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup and Next Steps with Async System\n",
    "\n",
    "Clean up resources and explore further research directions with the new async batch processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup async experiment resources\n",
    "if 'async_controller' in locals():\n",
    "    print(\"üßπ Cleaning up async experiment resources...\")\n",
    "    # The async controller automatically manages resources\n",
    "    print(\"‚úÖ Async resources cleaned up\")\n",
    "\n",
    "print(\"\\nüéâ Async Batch Processing Tutorial Completed Successfully!\")\n",
    "print(\"\\nüöÄ Next Steps with Async System:\")\n",
    "print(\"   1. Experiment with different batch configurations (Conservative/Balanced/Aggressive)\")\n",
    "print(\"   2. Scale up to larger populations (100-500 genomes) with async processing\")\n",
    "print(\"   3. Compare async vs sync performance on your specific use cases\")\n",
    "print(\"   4. Run comprehensive benchmarks using scripts/test_async_performance.py\")\n",
    "print(\"   5. Optimize batch sizes and concurrency for your API rate limits\")\n",
    "print(\"   6. Explore different models with async batch processing\")\n",
    "print(\"   7. Evaluate evolved prompts on larger problem sets efficiently\")\n",
    "\n",
    "print(\"\\nüìö Async System Resources:\")\n",
    "print(\"   - examples/async_evolution_example.py for complete async examples\")\n",
    "print(\"   - scripts/test_async_performance.py for performance benchmarking\")\n",
    "print(\"   - scripts/test_integration.py for system integration testing\")\n",
    "print(\"   - docs/async_batch_evaluation.md for detailed documentation\")\n",
    "print(\"   - src/genetics/async_evolution.py for async evolution implementation\")\n",
    "\n",
    "print(\"\\n‚ö° Performance Achievements:\")\n",
    "if 'speedup_factor' in locals():\n",
    "    print(f\"   üöÄ Achieved {speedup_factor:.1f}x speedup over sequential processing\")\n",
    "    print(f\"   üìà Throughput: {async_throughput:.1f} problems/second\")\n",
    "    print(f\"   ‚è±Ô∏è  Time saved: {(estimated_sync_time - total_experiment_time)/60:.1f} minutes\")\n",
    "else:\n",
    "    print(\"   üöÄ Expected 3-8x speedup over sequential processing\")\n",
    "    print(\"   üìà Expected throughput: 15-40 problems/second\")\n",
    "    print(\"   ‚è±Ô∏è  Significant time savings on large experiments\")\n",
    "\n",
    "print(\"\\nüåü The async batch processing system is now ready for production use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsm8k_ga_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
